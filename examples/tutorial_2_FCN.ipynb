{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# snnTorch Test - Truncated BPTT\n",
    "### By Jason K. Eshraghian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient-based Learning in Spiking Neural Networks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install the test PyPi Distribution of snntorch\n",
    "!pip install -i https://test.pypi.org/simple/ snntorch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Setting up the Static MNIST Dataset\n",
    "### 1.1. Import packages and setup environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Define network and SNN parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Network Architecture\n",
    "num_inputs = 28*28\n",
    "num_hidden = 1000\n",
    "num_outputs = 10\n",
    "\n",
    "# Training Parameters\n",
    "batch_size=128\n",
    "data_path='/data/mnist'\n",
    "\n",
    "# Temporal Dynamics\n",
    "num_steps = 25\n",
    "time_step = 1e-3\n",
    "tau_mem = 3e-3\n",
    "tau_syn = 2.2e-3\n",
    "alpha = float(np.exp(-time_step/tau_syn))\n",
    "beta = float(np.exp(-time_step/tau_mem))\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Download MNIST Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4 Create DataLoaders"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Define Network\n",
    "snnTorch treats neurons as activations with recurrent connections. This allows for smooth integration with PyTorch.\n",
    "There are a few useful neuron models and surrogate gradient functions which approximate the gradient of spikes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# spike_grad = snn.FastSigmoidSurrogate.apply\n",
    "# snn.slope = 50"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following network model no longer has a for-loop. That is performed in the feedforward pass along with optimization at each time step."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Stein(alpha=alpha, beta=beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Stein(alpha=alpha, beta=beta)\n",
    "\n",
    "    def forward(self, x, syn1, mem1, spk1, syn2, mem2):\n",
    "        cur1 = self.fc1(x)\n",
    "        spk1, syn1, mem1 = self.lif1(cur1, syn1, mem1)\n",
    "        cur2 = self.fc2(spk1)\n",
    "        spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\n",
    "        return syn1, mem1, spk1, syn2, mem2, spk2\n",
    "\n",
    "net = Net().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Training\n",
    "Time for training! Let's first define a couple of functions to print out test/train accuracy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "def print_batch_accuracy(data, targets, syn1, mem1, spk1, syn2, mem2, train=False):\n",
    "  spk2_rec = []\n",
    "  for step in range(num_steps):\n",
    "    syn1, mem1, spk1, syn2, mem2, spk2 = net(data.view(batch_size, -1), syn1, mem1, spk1, syn2, mem2)\n",
    "    spk2_rec.append(spk2)\n",
    "  spk2_rec = torch.stack(spk2_rec, dim=0)\n",
    "  _, idx = spk2_rec.sum(dim=0).max(1)\n",
    "  acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "\n",
    "  if train:\n",
    "      print(f\"Train Set Accuracy: {acc}\")\n",
    "  else:\n",
    "      print(f\"Test Set Accuracy: {acc}\")\n",
    "\n",
    "def train_printer(syn1, mem1, spk1, syn2, mem2, test_syn1, test_mem1, test_spk1, test_syn2, test_mem2):\n",
    "    print(f\"Epoch {epoch}, Minibatch {minibatch_counter}\")\n",
    "    print(f\"Train Set Loss: {loss_hist[counter]}\")\n",
    "    print(f\"Test Set Loss: {test_loss_hist[counter]}\")\n",
    "    print_batch_accuracy(data_it, targets_it, syn1, mem1, spk1, syn2, mem2, train=True)\n",
    "    print_batch_accuracy(testdata_it, testtargets_it, test_syn1, test_mem1, test_spk1, test_syn2, test_mem2, train=False)\n",
    "    print(\"\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Training Loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-4, betas=(0.9, 0.999))\n",
    "log_softmax_fn = nn.LogSoftmax(dim=-1)\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "test_data = itertools.cycle(test_loader)\n",
    "\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "counter = 0\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(5):\n",
    "    train_batch = iter(train_loader)\n",
    "\n",
    "    # Minibatch training loop\n",
    "    minibatch_counter = 0\n",
    "    for data_it, targets_it in train_batch:\n",
    "        data_it = data_it.to(device)\n",
    "        targets_it = targets_it.to(device)\n",
    "\n",
    "        # Test set iterator\n",
    "        testdata_it, testtargets_it = next(test_data)\n",
    "        testdata_it = testdata_it.to(device)\n",
    "        testtargets_it = testtargets_it.to(device)\n",
    "\n",
    "        # initialization\n",
    "        spk1, syn1, mem1 = net.lif1.init_stein(batch_size, num_hidden)\n",
    "        spk2, syn2, mem2 = net.lif2.init_stein(batch_size, num_outputs)\n",
    "\n",
    "        # test: initialization\n",
    "        test_spk1, test_syn1, test_mem1 = net.lif1.init_stein(batch_size, num_hidden)\n",
    "        test_spk2, test_syn2, test_mem2 = net.lif2.init_stein(batch_size, num_outputs)\n",
    "\n",
    "        # training loop\n",
    "        step_counter = 0\n",
    "        for steps in range(num_steps):\n",
    "            syn1, mem1, spk1, syn2, mem2, spk2 = net(data_it.view(batch_size, -1), syn1, mem1, spk1, syn2, mem2)\n",
    "\n",
    "            # loss p/timestep --- can try truncated approach too\n",
    "            log_p_y = log_softmax_fn(mem2) # mem2 = 128 x 10\n",
    "            loss_val = loss_fn(log_p_y, targets_it) # targets_it = 128\n",
    "            loss_hist.append(loss_val.item())\n",
    "\n",
    "            # Gradient calculation - detach states so gradient can flow\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "\n",
    "            # Weight Update\n",
    "            # nn.utils.clip_grad_norm_(net.parameters(), 1) # gradient clipping\n",
    "            optimizer.step()\n",
    "\n",
    "            # Detach for next update - test which of these variables don't have to be detached\n",
    "            syn1.detach_()\n",
    "            mem1.detach_()\n",
    "            spk1.detach_()\n",
    "            syn2.detach_()\n",
    "            mem2.detach_()\n",
    "\n",
    "            # Test set forward pass\n",
    "            test_syn1, test_mem1, test_spk1, test_syn2, test_mem2, test_spk2  = net(testdata_it.view(batch_size, -1), test_syn1, test_mem1, test_spk1, test_syn2, test_mem2) ### WAY TOO MANY VARS\n",
    "\n",
    "            # Test set loss\n",
    "            log_p_ytest = log_softmax_fn(test_mem2)\n",
    "            loss_val_test = loss_fn(log_p_ytest, testtargets_it)\n",
    "            test_loss_hist.append(loss_val_test.item())\n",
    "\n",
    "\n",
    "            # Print test/train loss/accuracy\n",
    "            if step_counter == 24:\n",
    "                train_printer(syn1, mem1, spk1, syn2, mem2, test_syn1, test_mem1, test_spk1, test_syn2, test_mem2) ## THIS IS A JOKE\n",
    "            step_counter += 1\n",
    "            counter +=1\n",
    "\n",
    "        minibatch_counter += 1\n",
    "\n",
    "loss_hist_true_grad = loss_hist\n",
    "test_loss_hist_true_grad = test_loss_hist"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Results\n",
    "### 4.1 Plot Training/Test Loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "plt.plot(loss_hist)\n",
    "plt.plot(test_loss_hist)\n",
    "plt.legend([\"Train Loss\", \"Test Loss\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 Test Set Accuracy\n",
    "This function just iterates over all minibatches to obtain a measure of accuracy over the full 10,000 samples in the test set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "  net.eval()\n",
    "  for data in test_loader:\n",
    "    images, labels = data\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # If current batch matches batch_size, just do the usual thing\n",
    "    if images.size()[0] == batch_size:\n",
    "      outputs, _ = net(images.view(batch_size, -1))\n",
    "\n",
    "    # If current batch does not match batch_size (e.g., is the final minibatch),\n",
    "    # modify batch_size in a temp variable and restore it at the end of the else block\n",
    "    else:\n",
    "      temp_bs = batch_size\n",
    "      batch_size = images.size()[0]\n",
    "      outputs, _ = net(images.view(images.size()[0], -1))\n",
    "      batch_size = temp_bs\n",
    "\n",
    "    _, predicted = outputs.sum(dim=0).max(1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Total correctly classified test set images: {correct}/{total}\")\n",
    "print(f\"Test Set Accuracy: {100 * correct / total}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Voila! That's it for static MNIST."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Spiking MNIST\n",
    "Part of the appeal of SNNs is their ability to handle time-varying spiking data. So let's use rate-coding to convert MNIST into spiking MNIST using the `spikegen` module in the previous tutorial, and train our network with that instead."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from snntorch import spikegen\n",
    "\n",
    "# MNIST to spiking-MNIST\n",
    "spike_data, spike_targets = spikegen.rate(data_it, targets_it, num_outputs=num_outputs, num_steps=num_steps, gain=1,\n",
    "                                          offset=0, convert_targets=False, temporal_targets=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1 Visualiser\n",
    "Just so you're damn sure it's a spiking input."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install celluloid # matplotlib animations made easy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from celluloid import Camera\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Animator\n",
    "spike_data_sample = spike_data[:, 0, 0].cpu()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "camera = Camera(fig)\n",
    "plt.axis('off')\n",
    "\n",
    "for step in range(num_steps):\n",
    "    im = ax.imshow(spike_data_sample[step, :, :], cmap='plasma')\n",
    "    camera.snap()\n",
    "\n",
    "# interval=40 specifies 40ms delay between frames\n",
    "a = camera.animate(interval=40)\n",
    "HTML(a.to_html5_video())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(spike_targets[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Define Network\n",
    "The network is the same as before. The one difference is that the for-loop iterates through the first dimension of the input:\n",
    "`cur1 = self.fc1(x[step])`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spike_grad = snn.FastSigmoidSurrogate.apply\n",
    "snn.slope = 50 # The lower the slope, the smoother the gradient\n",
    "\n",
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden states + output spike at t=0\n",
    "        spk1, syn1, mem1 = self.lif1.init_stein(batch_size, num_hidden)\n",
    "        spk2, syn2, mem2 = self.lif2.init_stein(batch_size, num_outputs)\n",
    "\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            cur1 = self.fc1(x[step])\n",
    "            spk1, syn1, mem1 = self.lif1(cur1, syn1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\n",
    "\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
    "\n",
    "net = Net().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Training\n",
    "We make a slight modification to our print-out functions to handle the new first dimension of the input:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_batch_accuracy(data, targets, train=False):\n",
    "    output, _ = net(data.view(num_steps, batch_size, -1))\n",
    "    _, idx = output.sum(dim=0).max(1)\n",
    "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "\n",
    "    if train:\n",
    "        print(f\"Train Set Accuracy: {acc}\")\n",
    "    else:\n",
    "        print(f\"Test Set Accuracy: {acc}\")\n",
    "\n",
    "def train_printer():\n",
    "    print(f\"Epoch {epoch}, Minibatch {minibatch_counter}\")\n",
    "    print(f\"Train Set Loss: {loss_hist[counter]}\")\n",
    "    print(f\"Test Set Loss: {test_loss_hist[counter]}\")\n",
    "    print_batch_accuracy(spike_data, spike_targets, train=True)\n",
    "    print_batch_accuracy(test_spike_data, test_spike_targets, train=False)\n",
    "    print(\"\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.1 Optimizer & Loss\n",
    "We'll keep our optimizer and loss the exact same as the static MNIST case."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-4, betas=(0.9, 0.999))\n",
    "log_softmax_fn = nn.LogSoftmax(dim=-1)\n",
    "loss_fn = nn.NLLLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.2 Training Loop\n",
    "The training loop is identical to the static MNIST case, but we pass each minibatch through `spikegen.rate` before running it through the feedforward network."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "counter = 0\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(3):\n",
    "    minibatch_counter = 0\n",
    "    data = iter(train_loader)\n",
    "\n",
    "    # Minibatch training loop\n",
    "    for data_it, targets_it in data:\n",
    "        data_it = data_it.to(device)\n",
    "        targets_it = targets_it.to(device)\n",
    "\n",
    "        # Spike generator\n",
    "        spike_data, spike_targets = spikegen.rate(data_it, targets_it, num_outputs=num_outputs, num_steps=num_steps,\n",
    "                                                  gain=1, offset=0, convert_targets=False, temporal_targets=False)\n",
    "\n",
    "        # Forward pass\n",
    "        output, mem_rec = net(spike_data.view(num_steps, batch_size, -1))\n",
    "        log_p_y = log_softmax_fn(mem_rec)\n",
    "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "\n",
    "        # Sum loss over time steps to perform BPTT\n",
    "        for step in range(num_steps):\n",
    "          loss_val += loss_fn(log_p_y[step], targets_it)\n",
    "\n",
    "        # Gradient Calculation\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward(retain_graph=True)\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
    "\n",
    "        # Weight Update\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store Loss history\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # Test set\n",
    "        test_data = itertools.cycle(test_loader)\n",
    "        testdata_it, testtargets_it = next(test_data)\n",
    "        testdata_it = testdata_it.to(device)\n",
    "        testtargets_it = testtargets_it.to(device)\n",
    "\n",
    "        # Test set spike conversion\n",
    "        test_spike_data, test_spike_targets = spikegen.rate(testdata_it, testtargets_it, num_outputs=num_outputs,\n",
    "                                                            num_steps=num_steps, gain=1, offset=0, convert_targets=False, temporal_targets=False)\n",
    "\n",
    "        # Test set forward pass\n",
    "        test_output, test_mem_rec = net(test_spike_data.view(num_steps, batch_size, -1))\n",
    "\n",
    "        # Test set loss\n",
    "        log_p_ytest = log_softmax_fn(test_mem_rec)\n",
    "        log_p_ytest = log_p_ytest.sum(dim=0)\n",
    "        loss_val_test = loss_fn(log_p_ytest, test_spike_targets)\n",
    "        test_loss_hist.append(loss_val_test.item())\n",
    "\n",
    "        # Print test/train loss/accuracy\n",
    "        if counter % 50 == 0:\n",
    "            train_printer()\n",
    "        minibatch_counter += 1\n",
    "        counter += 1\n",
    "\n",
    "loss_hist_true_grad = loss_hist\n",
    "test_loss_hist_true_grad = test_loss_hist"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Spiking MNIST Results\n",
    "### 8.1 Plot Training/Test Loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "plt.plot(loss_hist)\n",
    "plt.plot(test_loss_hist)\n",
    "plt.legend([\"Test Loss\", \"Train Loss\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8.2 Test Set Accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "  net.eval()\n",
    "  for data in test_loader:\n",
    "    images, labels = data\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # If current batch matches batch_size, just do the usual thing\n",
    "    if images.size()[0] == batch_size:\n",
    "      spike_test, spike_targets = spikegen.rate(images, labels, num_outputs=num_outputs, num_steps=num_steps,\n",
    "                                                            gain=1, offset=0, convert_targets=False, temporal_targets=False)\n",
    "\n",
    "      outputs, _ = net(spike_test.view(num_steps, batch_size, -1))\n",
    "\n",
    "    # If current batch does not match batch_size (e.g., is the final minibatch),\n",
    "    # modify batch_size in a temp variable and restore it at the end of the else block\n",
    "    else:\n",
    "      temp_bs = batch_size\n",
    "      batch_size = images.size()[0]\n",
    "      spike_test, spike_targets = spikegen.rate(images, labels, num_outputs=num_outputs, num_steps=num_steps,\n",
    "                                                gain=1, offset=0, convert_targets=False, temporal_targets=False)\n",
    "\n",
    "      outputs, _ = net(spike_test.view(num_steps, images.size()[0], -1))\n",
    "      batch_size = temp_bs\n",
    "\n",
    "    _, predicted = outputs.sum(dim=0).max(1)\n",
    "    total += spike_targets.size(0)\n",
    "    correct += (predicted == spike_targets).sum().item()\n",
    "\n",
    "print(f\"Total correctly classified test set images: {correct}/{total}\")\n",
    "print(f\"Test Set Accuracy: {100 * correct / total}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's all for now!\n",
    "Next time, we'll introduce how to use spiking convolutional layers to improve accuracy."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}