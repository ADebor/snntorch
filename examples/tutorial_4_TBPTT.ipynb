{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# snnTorch - Tutorial 4\n",
    "### By Jason K. Eshraghian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: explanation is yet to be added. For now, only code is updated to show how to automatically implement TBPTT.\n",
    "\n",
    "# Truncated Backpropagation through time\n",
    "In this tutorial, we'll use a convolutional neural network (CNN) to classify the MNIST dataset.\n",
    "We will use the truncated backpropagation through time (BPTT) algorithm to do so. This tutorial is largely the same as tutorial 2, just with a different network architecture to show how to integrate convolutions with snnTorch.\n",
    "\n",
    "If running in Google Colab:\n",
    "* Ensure you are connected to GPU by checking Runtime > Change runtime type > Hardware accelerator: GPU\n",
    "* Next, install the Test PyPi distribution of snnTorch by clicking into the following cell and pressing `Shift+Enter`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install the test PyPi Distribution of snntorch\n",
    "!pip install -i https://test.pypi.org/simple/ snntorch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Setting up the Static MNIST Dataset\n",
    "### 1.1. Import packages and setup environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "from snntorch import backprop as bp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Define network and SNN parameters\n",
    "We will use a 2conv-2MaxPool-FCN architecture for a sequence of 25 time steps.\n",
    "\n",
    "* `alpha` is the decay rate of the synaptic current of a neuron\n",
    "* `beta` is the decay rate of the membrane potential of a neuron"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Network Architecture\n",
    "num_inputs = 28*28\n",
    "num_hidden = 1000\n",
    "num_outputs = 10\n",
    "\n",
    "# Training Parameters\n",
    "batch_size=128\n",
    "data_path='/data/mnist'\n",
    "\n",
    "# Temporal Dynamics\n",
    "num_steps = 25\n",
    "time_step = 1e-3\n",
    "tau_mem = 4e-3\n",
    "tau_syn = 3e-3\n",
    "alpha = float(np.exp(-time_step/tau_syn))\n",
    "beta = float(np.exp(-time_step/tau_mem))\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Download MNIST Dataset\n",
    "To see how to construct a validation set, refer to Tutorial 1."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4 Create DataLoaders"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Define Network\n",
    "snnTorch contains a series of neuron models and related functions to ease the training process.\n",
    "Neurons are treated as activations with recurrent connections, and integrate smoothly with PyTorch's pre-existing layer functions.\n",
    "* `snntorch.Stein` is a simple Leaky Integrate and Fire (LIF) neuron. Specifically, it uses Stein's model which assumes instantaneous rise times for synaptic current and membrane potential.\n",
    "* `snntorch.FastSigmoidSurrogate` defines separate forward and backward functions. The forward function is a Heaviside step function for spike generation. The backward function is the derivative of a fast sigmoid function, to ensure continuous differentiability.\n",
    "FSS is mostly derived from:\n",
    "\n",
    ">Neftci, E. O., Mostafa, H., and Zenke, F. (2019) Surrogate Gradient Learning in Spiking Neural Networks. https://arxiv.org/abs/1901/09948\n",
    "\n",
    "There are a few other surrogate gradient functions included.\n",
    "`snn.slope` is a variable that defines the slope of the backward surrogate.\n",
    "TO-DO: Include visualisation.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can define our spiking neural network (SNN).\n",
    "If you have already worked through Tutorial 2, you may wish to skip ahead.\n",
    "\n",
    "The hidden_init argument will initialize the hidden states & spike outputs as instance variables.\n",
    "Although the forward method looks messier with the calls to these instance variables, it eliminates the need to detach all the variables from the computational graph manually."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # initialize layers\n",
    "        snn.LIF.clear_instances() # boilerplate\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Stein(alpha=alpha, beta=beta, num_inputs=num_hidden, batch_size=batch_size, hidden_init=True)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Stein(alpha=alpha, beta=beta, num_inputs=num_outputs, batch_size=batch_size, hidden_init=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        cur1 = self.fc1(x)\n",
    "        self.lif1.spk1, self.lif1.syn1, self.lif1.mem1 = self.lif1(cur1, self.lif1.syn, self.lif1.mem)\n",
    "        cur2 = self.fc2(self.lif1.spk)\n",
    "        self.lif2.spk, self.lif2.syn, self.lif2.mem = self.lif2(cur2, self.lif2.syn, self.lif2.mem)\n",
    "\n",
    "        return self.lif2.spk, self.lif2.mem\n",
    "\n",
    "net = Net().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Training\n",
    "Time for training! Let's define our train and test functions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def train(net, device, train_loader, optimizer, criterion, epoch):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        loss = bp.TBPTT(net, data, target, num_steps, batch_size, optimizer, criterion, K=K)\n",
    "        # loss = bp.BPTT(net, data, target, num_steps, batch_size, optimizer, criterion)\n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx*len(data)}/{len(train_loader.dataset)}], \"\n",
    "                  f\"Loss: {loss.item()}\")\n",
    "    loss_hist.append(loss.item())  # only recording at the end of each epoch\n",
    "\n",
    "\n",
    "def test(net, device, test_loader, criterion):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            spk2_rec = []\n",
    "            snn.Stein.zeros_hidden()  # reset hidden states to 0\n",
    "            if data.size()[0] == batch_size:\n",
    "                for step in range(num_steps):\n",
    "                    spk2, mem2 = net(data.view(batch_size, -1))\n",
    "                    spk2_rec.append(spk2)\n",
    "\n",
    "                # Test Loss where batch=128; only calc on final time step\n",
    "                # log_p_ytest = log_softmax_fn(mem2)\n",
    "                test_loss += criterion(mem2, target)\n",
    "                # Test Acc where batch=128\n",
    "                _, idx = torch.stack(spk2_rec, dim=0).sum(dim=0).max(1)  # predicted indexes\n",
    "                correct += sum((target == idx).cpu().numpy())\n",
    "                # print(correct)\n",
    "\n",
    "            else:  # Handle drop_last = False\n",
    "                temp_data = torch.zeros((batch_size, *(data[0].size())), dtype=dtype, device=device)  # pad out temp_data now\n",
    "                temp_data[:(data.size()[0])] = data\n",
    "\n",
    "                for step in range(num_steps):\n",
    "                    spk2, mem2 = net(temp_data.view(batch_size, -1))\n",
    "                    spk2_rec.append(spk2)\n",
    "\n",
    "                # Test set loss - only calc on the final time-step\n",
    "                # log_p_ytest = log_softmax_fn(mem2[:data.size()[0]])\n",
    "                test_loss += criterion(mem2[:data.size()[0]], target)\n",
    "                # Test Acc where batch=128\n",
    "                _, idx = torch.stack(spk2_rec, dim=0).sum(dim=0).max(1)  # predicted indexes\n",
    "                correct += sum((target == idx[:data.size()[0]]).cpu().numpy())\n",
    "\n",
    "        test_loss_hist.append(test_loss.item())\n",
    "        test_acc = correct / len(test_loader.dataset)\n",
    "        print(f\"\\nTest set: Average loss: {(test_loss/(len(test_loader.dataset)/batch_size))}, Accuracy: [{correct}/{len(test_loader.dataset)}] ({(correct/len(test_loader.dataset))})\\n\"\n",
    "              f\"=====================\\n\")\n",
    "\n",
    "        return test_loss, test_acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Optimizer & Loss\n",
    "* *Output Activation*: We'll apply the softmax function to the membrane potentials of the output layer, rather than the spikes.\n",
    "* *Loss*: This will then be used to calculate the negative log-likelihood loss.\n",
    "By encouraging the membrane of the correct neuron class to reach the threshold, we expect that neuron will fire more frequently.\n",
    "The loss could be applied to the spike count as well, but the membrane is  continuous whereas spike count is discrete.\n",
    "* *Optimizer*: The Adam optimizer is used for weight updates.\n",
    "* *Accuracy*: Accuracy is measured by counting the spikes of the output neurons. The neuron that fires the most frequently will be our predicted class.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Training Loop\n",
    "Now just sit back, relax, and wait for convergence."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Trial: 0, Learning Rate: 0.0001\n",
      "Train Epoch: 0 [0/60000], Loss: 63.82491683959961\n",
      "Train Epoch: 0 [2560/60000], Loss: 26.916061401367188\n",
      "Train Epoch: 0 [5120/60000], Loss: 21.04145622253418\n",
      "Train Epoch: 0 [7680/60000], Loss: 17.500253677368164\n",
      "Train Epoch: 0 [10240/60000], Loss: 16.819568634033203\n",
      "Train Epoch: 0 [12800/60000], Loss: 16.37757682800293\n",
      "Train Epoch: 0 [15360/60000], Loss: 16.062217712402344\n",
      "Train Epoch: 0 [17920/60000], Loss: 15.913599967956543\n",
      "Train Epoch: 0 [20480/60000], Loss: 15.497381210327148\n",
      "Train Epoch: 0 [23040/60000], Loss: 13.099267959594727\n",
      "Train Epoch: 0 [25600/60000], Loss: 12.441390037536621\n",
      "Train Epoch: 0 [28160/60000], Loss: 13.864492416381836\n",
      "Train Epoch: 0 [30720/60000], Loss: 14.410299301147461\n",
      "Train Epoch: 0 [33280/60000], Loss: 15.119754791259766\n",
      "Train Epoch: 0 [35840/60000], Loss: 16.029226303100586\n",
      "Train Epoch: 0 [38400/60000], Loss: 11.700634956359863\n",
      "Train Epoch: 0 [40960/60000], Loss: 10.631417274475098\n",
      "Train Epoch: 0 [43520/60000], Loss: 10.795496940612793\n",
      "Train Epoch: 0 [46080/60000], Loss: 12.09965705871582\n",
      "Train Epoch: 0 [48640/60000], Loss: 16.178878784179688\n",
      "Train Epoch: 0 [51200/60000], Loss: 15.34487247467041\n",
      "Train Epoch: 0 [53760/60000], Loss: 12.431215286254883\n",
      "Train Epoch: 0 [56320/60000], Loss: 10.978805541992188\n",
      "Train Epoch: 0 [58880/60000], Loss: 10.478025436401367\n",
      "\n",
      "Test set: Average loss: 0.25882551074028015, Accuracy: [9199/10000] (0.9199)\n",
      "=====================\n",
      "\n",
      "Train Epoch: 1 [0/60000], Loss: 11.284976959228516\n",
      "Train Epoch: 1 [2560/60000], Loss: 11.43734073638916\n",
      "Train Epoch: 1 [5120/60000], Loss: 11.253931999206543\n",
      "Train Epoch: 1 [7680/60000], Loss: 15.115819931030273\n",
      "Train Epoch: 1 [10240/60000], Loss: 13.283331871032715\n",
      "Train Epoch: 1 [12800/60000], Loss: 11.285038948059082\n",
      "Train Epoch: 1 [15360/60000], Loss: 10.965374946594238\n",
      "Train Epoch: 1 [17920/60000], Loss: 11.924263000488281\n",
      "Train Epoch: 1 [20480/60000], Loss: 11.847746849060059\n",
      "Train Epoch: 1 [23040/60000], Loss: 9.841207504272461\n",
      "Train Epoch: 1 [25600/60000], Loss: 10.947335243225098\n",
      "Train Epoch: 1 [28160/60000], Loss: 15.245718002319336\n",
      "Train Epoch: 1 [30720/60000], Loss: 10.990610122680664\n",
      "Train Epoch: 1 [33280/60000], Loss: 11.220377922058105\n",
      "Train Epoch: 1 [35840/60000], Loss: 9.993741989135742\n",
      "Train Epoch: 1 [38400/60000], Loss: 10.114492416381836\n",
      "Train Epoch: 1 [40960/60000], Loss: 11.540345191955566\n",
      "Train Epoch: 1 [43520/60000], Loss: 11.040321350097656\n",
      "Train Epoch: 1 [46080/60000], Loss: 9.019800186157227\n",
      "Train Epoch: 1 [48640/60000], Loss: 10.694330215454102\n",
      "Train Epoch: 1 [51200/60000], Loss: 9.6201753616333\n",
      "Train Epoch: 1 [53760/60000], Loss: 11.086962699890137\n",
      "Train Epoch: 1 [56320/60000], Loss: 10.326557159423828\n",
      "Train Epoch: 1 [58880/60000], Loss: 12.990628242492676\n",
      "\n",
      "Test set: Average loss: 0.18386633694171906, Accuracy: [9346/10000] (0.9346)\n",
      "=====================\n",
      "\n",
      "Train Epoch: 2 [0/60000], Loss: 10.70390510559082\n",
      "Train Epoch: 2 [2560/60000], Loss: 8.949257850646973\n",
      "Train Epoch: 2 [5120/60000], Loss: 10.340015411376953\n",
      "Train Epoch: 2 [7680/60000], Loss: 9.764623641967773\n",
      "Train Epoch: 2 [10240/60000], Loss: 12.002951622009277\n",
      "Train Epoch: 2 [12800/60000], Loss: 8.352147102355957\n",
      "Train Epoch: 2 [15360/60000], Loss: 10.065727233886719\n",
      "Train Epoch: 2 [17920/60000], Loss: 8.556246757507324\n",
      "Train Epoch: 2 [20480/60000], Loss: 10.30932331085205\n",
      "Train Epoch: 2 [23040/60000], Loss: 10.191876411437988\n",
      "Train Epoch: 2 [25600/60000], Loss: 9.987879753112793\n",
      "Train Epoch: 2 [28160/60000], Loss: 11.800651550292969\n",
      "Train Epoch: 2 [30720/60000], Loss: 10.976938247680664\n",
      "Train Epoch: 2 [33280/60000], Loss: 10.62382984161377\n",
      "Train Epoch: 2 [35840/60000], Loss: 8.914140701293945\n"
     ]
    }
   ],
   "source": [
    "no_trials = 1\n",
    "# lr_values = [1e-3, 5e-4, 1e-4] # these values are good\n",
    "lr_values = [1e-4]\n",
    "batch_size = 128\n",
    "data_path = '/data/mnist'\n",
    "# subset = 50  # can remove this line in Colab\n",
    "num_steps = 25\n",
    "epochs = 25\n",
    "betas = (0.9, 0.999)\n",
    "K = 25  # number of time steps to accumulate over -- right now I'm using BPTT anyway so this is ignored\n",
    "SAVE_GOOGLE_COLAB = False\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0,), (1,))\n",
    "])\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
    "# mnist_train = data_subset(mnist_train, subset)  # reduce dataset by x100 - can remove this line in Colab\n",
    "# mnist_test = data_subset(mnist_test, subset)\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "# Adam\n",
    "# df = pd.DataFrame(columns=['lr', 'epoch', 'test_set_loss', 'test_set_accuracy'])\n",
    "for i in range(no_trials):\n",
    "    for lr in lr_values:\n",
    "        net = Net().to(device)\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=betas)\n",
    "        # log_softmax_fn = nn.LogSoftmax(dim=-1)\n",
    "        criterion = nn.CrossEntropyLoss() # note: CrossEntropy dims must be B x num_classes. Can increase dimensionality, read docs.\n",
    "\n",
    "        loss_hist = []\n",
    "        test_loss_hist = []\n",
    "        print(f\"========Trial: {i}, Learning Rate: {lr}\")\n",
    "        for epoch in range(epochs):\n",
    "            train(net, device, train_loader, optimizer, criterion, epoch)\n",
    "            test_loss, test_acc = test(net, device, test_loader, criterion)\n",
    "\n",
    "            # df = df.append(\n",
    "            #     {'trial': i, 'lr': lr, 'epoch': epoch, 'test_set_loss': test_loss.item(),\n",
    "            #      'test_set_accuracy': test_acc}, ignore_index=True)\n",
    "            # df.to_csv('Adam_BPTT2.csv', index=False)\n",
    "            # if SAVE_GOOGLE_COLAB:\n",
    "            #     shutil.copy(\"Adam_BPTT.csv\", \"/content/Adam_BPTT.csv\")\n",
    "\n",
    "\n",
    "loss_hist_true_grad = loss_hist\n",
    "test_loss_hist_true_grad = test_loss_hist"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Results\n",
    "### 4.1 Plot Training/Test Loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "plt.plot(loss_hist)\n",
    "plt.plot(test_loss_hist)\n",
    "plt.legend([\"Test Loss\", \"Train Loss\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's it for static MNIST!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}