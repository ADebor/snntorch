{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "tutorial_2_FCN_truncatedfromscratch.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/snntorch_alpha_w.png?raw=true' width=\"400\">\n",
        "\n",
        "# snnTorch - Deep Learning with ``snntorch``\n",
        "## Tutorial 3\n",
        "### By Jason K. Eshraghian (www.jasoneshraghian.com)\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/jeshraghian/snntorch/blob/tutorials/examples/tutorial_3_FCN.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "uSGZ6cdmpknm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\r\n",
        "In this tutorial, you will:\r\n",
        "* Learn how spiking neurons are implemented in a recurrent network\r\n",
        "* Understand backpropagation through time, and the associated challenges in SNNs such as target labeling, and the non-differentiability of spikes\r\n",
        "* Train a fully-connected network on the static MNIST dataset\r\n",
        "\r\n",
        "<!-- * Implement various backprop strategies:\r\n",
        "  * Backpropagation Through Time\r\n",
        "  * Truncated-Backpropagation Through Time\r\n",
        "  * Real-Time Recurrent Learning -->\r\n",
        "\r\n",
        ">Part of this tutorial was inspired by Friedemann Zenke's extensive work on SNNs. Check out his repo on surrogate gradients [here](https://github.com/fzenke/spytorch), and a favourite paper of mine: E. O. Neftci, H. Mostafa, F. Zenke, [Surrogate Gradient Learning in Spiking Neural Networks: Bringing the Power of Gradient-based optimization to spiking neural networks.](https://ieeexplore.ieee.org/document/8891809) IEEE Signal Processing Magazine 36, 51â€“63.\r\n",
        "\r\n",
        "As a quick recap, [Tutorial 1](https://colab.research.google.com/github/jeshraghian/snntorch/blob/tutorials/examples/tutorial_1_spikegen.ipynb) explained how to convert datasets into spikes using three encoding mechanisms:\r\n",
        "* Rate coding\r\n",
        "* Latency coding\r\n",
        "* Delta modulation\r\n",
        "\r\n",
        "[Tutorial 2](https://colab.research.google.com/github/jeshraghian/snntorch/blob/tutorials/examples/tutorial_2_neuronal_dynamics.ipynb) showed how to build neural networks using three different leaky integrate-and-fire (LIF) neuron models:\r\n",
        "* Lapicque's RC model\r\n",
        "* Synaptic Conductance-based model\r\n",
        "* Alpha neuron model\r\n",
        "\r\n",
        "At the end of the tutorial, a basic supervised learning algorithm will be implemented. We will use the original static MNIST dataset and train a multi-layer fully-connected spiking neural network using gradient descent to perform image classification. \r\n",
        "\r\n",
        "If running in Google Colab:\r\n",
        "* You may connect to GPU by checking `Runtime` > `Change runtime type` > `Hardware accelerator: GPU`\r\n",
        "* Next, install the latest PyPi distribution of snnTorch by clicking into the following cell and pressing `Shift+Enter`."
      ],
      "metadata": {
        "id": "Ymi3sqJg28OQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install snntorch"
      ],
      "outputs": [],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "5tn_wUlopkon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. A Recurrent Representation of SNNs"
      ],
      "metadata": {
        "id": "gt2xMbLY9dVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is a summary of the continuous time-domain representation LIF neurons, and applies the result to develop a recurrent representation that is more suitable for use in recurrent neural networks (RNNs). \n",
        "\n",
        "We derived the dynamics of the passive membrane using an RC circuit in the time-domain: \n",
        "\n",
        "$$ \\tau_{\\rm mem} \\frac{dU_{\\rm mem}(t)}{dt} = -U_{\\rm mem}(t) + RI_{\\rm syn}(t),$$\n",
        "\n",
        "where the general solution of this equation is:\n",
        "\n",
        "$$U_{\\rm mem}=I_{\\rm syn}(t)R + [U_0 - I_{\\rm syn}(t)R]e^{-t/\\tau_{\\rm mem}}$$\n",
        "\n",
        "In Lapicque's model, $I_{\\rm syn}(t)$ is also the input current, $I_{\\rm in}(t)$. \n",
        "\n",
        "In the Synaptic conductance-based model (which we will loosely refer to as the synaptic model), a more biologically plausible approach is taken that ensures $I_{\\rm syn}(t)$ follows an exponential decay as a function of the input:\n",
        "\n",
        "\n",
        "$$I_{\\rm syn}(t) = \\sum_k W_{i,j} S_{in; i,j}(t) e^{-(t-t_k)/\\tau_{syn}}\\Theta(t-t_k)$$\n",
        "\n",
        "<center>\n",
        "<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial3/3_1_stein_decomp.png?raw=true' width=\"600\">\n",
        "</center>\n",
        "\n",
        "The synaptic model has two exponentially decaying terms: $I_{\\rm syn}(t)$ and $U_{\\rm mem}(t)$. The ratio between subsequent terms (i.e., decay rate) of $I_{\\rm syn}(t)$ is set to $\\alpha$, and that of $U_{\\rm mem}(t)$ is set to $\\beta$:\n",
        "\n",
        "$$ \\alpha = e^{-1/\\tau_{\\rm syn}}$$\n",
        "\n",
        "$$ \\beta = e^{-1/\\tau_{\\rm mem}}$$\n",
        "\n",
        "\n",
        "RNNs will process data sequentially, and so time must be discretised, and the neuron models must be converted into a recursive form. $\\alpha$ and $\\beta$ can be used to give a recursive representation of the Synaptic neuron model:\n",
        "\n",
        "$$I_{\\rm syn}[t+1]=\\underbrace{\\alpha I_{\\rm syn}[t]}_\\text{decay} + \\underbrace{WS_{\\rm in}[t+1]}_\\text{input}$$\n",
        "\n",
        "$$U[t+1] = \\underbrace{\\beta U[t]}_\\text{decay} + \\underbrace{I_{\\rm syn}[t+1]}_\\text{input} - \\underbrace{R[t+1]}_\\text{reset}$$\n",
        "\n",
        "**Spiking**\n",
        "\n",
        "If $U[t] > U_{\\rm thr}$, then an output spike is triggered: $S_{\\rm out}[t] = 1$. Otherwise, $S_{\\rm out}[t] = 0$. \n",
        "\n",
        "> Note: A variation of this is to set the output spike at the *next* time step to be triggered; i.e., $U[t] > U_{\\rm thr} \\implies S_{\\rm out}[t+1] = 1$. This is the approach taken in snnTorch, and will be explained in following sections.\n",
        "\n",
        "An alternative way to represent the relationship between $S_{\\rm out}$ and $U_{\\rm mem}$, which is also used to calculate the gradient in the backward pass, is:\n",
        "\n",
        "$$S_{\\rm out}[t] = \\Theta(U_{\\rm mem}[t] - U_{\\rm thr})$$ \n",
        "\n",
        "\n",
        "<center>\n",
        "<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial3/3_2_spike_descrip.png?raw=true' width=\"600\">\n",
        "</center>\n",
        "\n",
        "**Reset**\n",
        "\n",
        "The reset term is activated only when the neuron triggers a spike. That is to say, if $S_{\\rm out}[t+1]=1$:\n",
        "\n",
        "  * For `reset_mechanism=\"subtract\"`: $R[t+1]=U_{\\rm thr}$ \n",
        "  * For `reset_mechanism=\"zero\"`: $R[t+1]=U[t+1]$\n",
        "\n",
        "> Note: In snnTorch, the reset will also take a one time step delay such that $R[t+1]$ is activated only when $S_{\\rm out}[t+1]=1$\n",
        "\n",
        "The other neurons follow a similar form, which is [detailed in the documentation](https://snntorch.readthedocs.io/en/latest/snntorch.html). The recursive neuron equations can be mapped into computation graphs, where the recurrent connections take place with a delay of a single time step, from the state at time $t$ to the state at time $t+1$. "
      ],
      "metadata": {
        "id": "v7haBG7nA_TC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An alternative way to represent recurrent models is to unfold the computational graph, in which each component is represented by a sequence of different variables, with one variable per time step. The unfolded form of the Synaptic model is shown below:\n",
        "\n",
        "<center>\n",
        "<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial3/3_2_unrolled.png?raw=true' width=\"800\">\n",
        "</center>\n",
        "\n",
        "\n",
        "Up until now, the notation used for all variables have had an association with their electrical meanings. As we move from neuronal dynamics to deep learning, we will slightly modify the notation throughout the rest of the tutorial:\n",
        "\n",
        "* **Input spike:** $S_{\\rm in} \\rightarrow X$\n",
        "* **Input current (weighted spike):** $I_{\\rm in} \\rightarrow Y$\n",
        "* **Synaptic current:** $I_{\\rm syn} \\rightarrow I$\n",
        "* **Membrane potential:** $U_{\\rm mem} \\rightarrow U$\n",
        "* **Output spike:** $S_{\\rm out} \\rightarrow S$\n",
        "\n",
        "The benefit of an unrolled graph is that we now have an explicit description of how computations are performed. The process of unfolding illustrates the flow of information forward in time (from left to right) to compute outputs and losses, and backward in time to compute gradients. The more time steps that are simulated, the deeper the graph becomes. \n",
        "\n",
        "Conventional RNNs treat $\\alpha$ and $\\beta$ as learnable parameters. This is also possible for SNNs, but in snnTorch, they are treated as hyperparameters by default. This replaces the vanishing and exploding gradient problems with a parameter search."
      ],
      "metadata": {
        "id": "t256yMrzTU6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Setting up the Static MNIST Dataset"
      ],
      "metadata": {
        "id": "zqJdfllYbc16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Much of the following code has already been explained in the first two tutorials. So we'll dive straight in. "
      ],
      "metadata": {
        "id": "SuOWJNEMe8l_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Import packages and setup the environment"
      ],
      "metadata": {
        "id": "nbunSP5TbikZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "import snntorch as snn\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torchvision import datasets, transforms\r\n",
        "import numpy as np\r\n",
        "import itertools\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "outputs": [],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bEFWu3nNpkoq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "# Network Architecture\r\n",
        "num_inputs = 28*28\r\n",
        "num_hidden = 1000\r\n",
        "num_outputs = 10\r\n",
        "\r\n",
        "# Training Parameters\r\n",
        "batch_size=128\r\n",
        "data_path='/data/mnist'\r\n",
        "\r\n",
        "# Temporal Dynamics\r\n",
        "num_steps = 25\r\n",
        "alpha = 0.7\r\n",
        "beta = 0.8\r\n",
        "\r\n",
        "dtype = torch.float\r\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "outputs": [],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "lI0GbgLgpkos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Download MNIST Dataset"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "nUS6YFXbpkos"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "# Define a transform\r\n",
        "transform = transforms.Compose([\r\n",
        "            transforms.Resize((28, 28)),\r\n",
        "            transforms.Grayscale(),\r\n",
        "            transforms.ToTensor(),\r\n",
        "            transforms.Normalize((0,), (1,))])\r\n",
        "\r\n",
        "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\r\n",
        "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)"
      ],
      "outputs": [],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "2fhRixcspkot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the above code blocks throws an error, e.g. the MNIST servers are down, then uncomment the following code instead."
      ],
      "metadata": {
        "id": "RAM_dP887uTq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# # temporary dataloader if MNIST service is unavailable\r\n",
        "# !wget www.di.ens.fr/~lelarge/MNIST.tar.gz\r\n",
        "# !tar -zxvf MNIST.tar.gz\r\n",
        "\r\n",
        "# mnist_train = datasets.MNIST(root = './', train=True, download=True, transform=transform)\r\n",
        "# mnist_test = datasets.MNIST(root = './', train=False, download=True, transform=transform)"
      ],
      "outputs": [],
      "metadata": {
        "id": "4jyJVqUNdXDo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# Create DataLoaders\r\n",
        "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\r\n",
        "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)"
      ],
      "outputs": [],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "aEtCbO6upkou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Define the Network"
      ],
      "metadata": {
        "id": "GhFyzySNeT_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The spiking neurons available in snnTorch are designed to be treated as activation units. The only difference is that these spiking neuron activations depend not only on their inputs, but also on their previous state (e.g., $I[t-1]$ and $U[t-1]$ for the Synaptic neuron). This can be implemented in a for-loop with ease.\n",
        "\n",
        "If you have a basic understanding of PyTorch, the following code block should look familiar. `nn.Linear` initializes the linear transformation layer, and instead of applying a sigmoid, ReLU or some other nonlinear activation, a spiking neuron is applied instead by calling `snn.Synaptic`:"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "RJkoAg-3pkow"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "# Define Network\r\n",
        "class Net(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        # Initialize layers\r\n",
        "        self.fc1 = nn.Linear(num_inputs, num_hidden)\r\n",
        "        self.lif1 = snn.Synaptic(alpha=alpha, beta=beta)\r\n",
        "        self.fc2 = nn.Linear(num_hidden, num_outputs)\r\n",
        "        self.lif2 = snn.Synaptic(alpha=alpha, beta=beta)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "\r\n",
        "        # Initialize hidden states and outputs at t=0\r\n",
        "        syn1, mem1 = self.lif1.init_synaptic(num_hidden, device=device)\r\n",
        "        syn2, mem2 = self.lif2.init_synaptic(num_outputs, device=device)\r\n",
        "        \r\n",
        "        # Record the final layer\r\n",
        "        spk2_rec = []\r\n",
        "        mem2_rec = []\r\n",
        "\r\n",
        "        for step in range(num_steps):\r\n",
        "            cur1 = self.fc1(x)\r\n",
        "            spk1, syn1, mem1 = self.lif1(cur1, syn1, mem1)\r\n",
        "            cur2 = self.fc2(spk1)\r\n",
        "            spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\r\n",
        "\r\n",
        "            spk2_rec.append(spk2)\r\n",
        "            mem2_rec.append(mem2)\r\n",
        "\r\n",
        "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)"
      ],
      "outputs": [],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "-uquHLLmpkox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code in the `forward()` function will only be called once the input argument `x` is explicitly passed in:\n",
        "\n",
        "* `fc1` applies a linear transformation to the input: $W_{i, j}^{[1]}X_{i}^{[1]}[t] \\rightarrow Y_{j}^{[1]}[t]$, i.e., `cur1`\n",
        "* `lif1` integrates $Y^{[1]}_{j}[t]$ over time (with a decay), to generate $I_{j}^{[1]}[t]$ and $U_{j}^{[1]}[t]$. An output spike is triggered if $U_{j}^{[1]}[t] > U_{\\rm thr}$. Equivalently, `spk1=1` if `mem1` > `threshold=1.0`\n",
        "* `fc2` applies a linear transformation to `spk1`: $W_{j, k}^{[2]}S_{j}^{[1]}[t] \\rightarrow Y_{k}^{[2]}[t]$, i.e., `cur2`\n",
        "* `lif2` is another spiking neuron layer, and generates output spikes $S_{k}^{[2]}[t]$ which are returned in the variable `spk2`\n",
        "\n",
        "Here, $i$ denotes one of 784 input neurons, $j$ indexes one of the 1,000 neurons in the hidden layer, and $k$ points to one of 10 output neurons.\n"
      ],
      "metadata": {
        "id": "Y0fHcAKfrav6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The layers in `def __init__(self)` are automatically created upon instantiating `Net()`, as is done below:\n"
      ],
      "metadata": {
        "id": "3-yJXOSQqANb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "# Load the network onto CUDA if available\r\n",
        "net = Net().to(device)"
      ],
      "outputs": [],
      "metadata": {
        "id": "EkTWSXj5fj2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Backpropagation for SNNs"
      ],
      "metadata": {
        "id": "6a7MdORCtIx4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few questions arise when setting up a backprop-driven learning algorithm:\n",
        "\n",
        "1.   **Targets**: What should the target of the output layer be?\n",
        "2.   **Backprop through time**: How might the gradient flow back in time?\n",
        "3.   **Spike non-differentiability**: If spikes are discrete, instantaneous bursts of information, doesn't that make them non-differentiable? If the output spike has no gradient with respect to the network parameters, wouldn't backprop be impossible?\n",
        "\n",
        "Let's tackle these one by one. "
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "ZlrNIMNnpkoy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Target Labels"
      ],
      "metadata": {
        "id": "NpJhgA6n8LPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In [tutorial 1](https://colab.research.google.com/github/jeshraghian/snntorch/blob/tutorials/examples/tutorial_1_spikegen.ipynb), we learnt about rate and latency coding. Rate coding stores information in the frequency of spikes, and latency coding stores information in the timing of each spike. Previously, we used these encoding strategies to convert datasets into time-varying spikes. Here, they are used as encoding strategies for the output layer of our SNN. I.e., these codes will be used to teach the final layer of the network how to respond to certain inputs. \n",
        "\n",
        "The goal of the SNN is to predict a discrete variable with $n$ possible values, as is the case with MNIST where $n=10$. "
      ],
      "metadata": {
        "id": "k0At_jfe8OLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.1 Rate code"
      ],
      "metadata": {
        "id": "OnSx21wvAoee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For rate encoding, the most naive implementation is to encourage the correct class to fire at every time step, and the incorrect classes to not fire at all. There are two ways to implement this, one of which is a lot more effective than the other:\n",
        "\n",
        "* Set the target of the output spike of the correct class $y_{\\rm spk} = 1$ for all $t$, or\n",
        "* Set the target of the membrane potential of the correct class $y_{\\rm mem} = U_{\\rm thr}$ for all $t$ \n",
        "\n",
        "Which is the better approach? \n",
        "\n",
        "**Spiking Targets**\n",
        "\n",
        "Consider the first option. The output spikes are discrete events, and rely on large perturbations of the membrane potential around the threshold to have any infleunce. If the output spiking behavior goes unchanged, the gradient of the output of the network with respect to its parameters would be $0$. This is problematic, because the training process would no longer have a guide for how to improve the weights. It would be an ineffective approach for gradient descent. \n",
        "\n",
        "**Membrane Potential Targets**\n",
        "\n",
        "Instead, it is better to promote spiking by applying the target to the membrane potential. As the membrane potential is a much stronger function of the parameters, (i.e., a small perturbation of the weights would directly perturb the membrane potential), this would ensure there is a strong gradient whenever the network obtains a wrong result. So we set $y_{\\rm mem} = U_{\\rm thr}$. By default, `threshold=1`. The outputs can then be applied to a softmax unit, which are then used to find the cross-entropy loss:\n",
        "\n",
        "$$CE = - \\sum^n_{i=1}y_{i,\\rm mem} {\\rm log}(p_i),$$\n",
        "\n",
        "where $y_{i, \\rm mem}$ is the target label at a given time step, $n$ is the number of classes, and $p_i$ is the softmax probability for the $i^{th}$ class. \n",
        "\n",
        "The accuracy of the network would then be measured by counting up how many times each neuron fired across all time steps. We could then use `torch.max()` to choose the neuron with the most spikes, or somewhat equivalently, the highest average firing rate. \n",
        "\n",
        "It is possible to increase the target of membrane potential beyond the threshold to excite the neuron further. While this may be desirable in some instances, it will likely trigger high-conductance pathways for the wrong class when training other samples."
      ],
      "metadata": {
        "id": "4kR56xe3Ari3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Our classifier will implement the simplest form of rate coding. It will encourage the correct class to fire 100% of time steps, and the incorrect class to fire 0% of the time. Although this is clearly not the most efficient method, it is the simplest.*"
      ],
      "metadata": {
        "id": "b-xv_G0_304H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.2 Latency code\n"
      ],
      "metadata": {
        "id": "iknCYQEeCaHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In latency encoding, the neuron that fires first is the predicted class. The target may be set to 1 for one of the first few time steps. Depending on the neuron model being used, it will take several time steps before the input can propagate to the output of the network. Therefore, it is inadvisable to set the target to `1` only for the first time step. \r\n",
        "\r\n",
        "Consider the case of a neuron receiving an input spike. Depending on the neuron model in use, the post-synaptic potential may experience a time delay $t_{\\rm psp}$ to reach the peak of its membrane potential, and subsequently emit an output spike. If this neuron is connected in a deep neural network, the minimum time before the final layer could generate output spikes *as a result of the input (and not biases)* would thus be $t_{\\rm min} = Lt_{\\rm psp}$, where $L$ is the number of layers in the network. \r\n",
        "\r\n",
        "For the Synaptic and Lapicque models, the membrane potential will immediately jump as a result of the input. But there is a time delay of one step before the output spike can be triggered as a result. Therefore, we set $t_{\\rm psp}=1$ time step. For the Alpha neuron model, it will take a longer time to reach the peak, and is a function of the decay rates, $\\alpha$ and $\\beta$. \r\n",
        "\r\n",
        "<center>\r\n",
        "<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial3/3_3_delay.png?raw=true' width=\"450\">\r\n",
        "</center>\r\n",
        "\r\n",
        "In absence of this post-synaptic potential delay, it becomes challenging to control the output layer in terms of spike timing. An input spike of a multi-layer SNN could effectively be transmitted straight to the output instantaneously, without considering the input data at any later time steps. A slight modification is made to the unrolled computational graph, which adds a delay of one time step between $U$ and $S$.\r\n",
        "\r\n",
        "<center>\r\n",
        "<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial3/3_4_graphdelay.png?raw=true' width=\"700\">\r\n",
        "</center>"
      ],
      "metadata": {
        "id": "92hSKSBogC0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "As for the incorrect classes, it is acceptable to set their targets to 0. However, this could result in low conductance pathways that completely inhibit firing. It may be preferable to set their membrane potential target to something slightly higher, e.g., $U_{\\rm thr}/5$. The optimal point is a topic of further investigation. Note that all of the above can have a cross-entropy loss applied, just as with rate coding.\n",
        "\n",
        "A simple example across 4 time steps is provided in the image below, though the values and spiking periodicity should not be taken literally.\n",
        "\n",
        "<center>\n",
        "<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial3/3_5_targets.png?raw=true' width=\"700\">\n",
        "</center>\n",
        "\n",
        "An alternative approach is to treat the number of time steps as a continuous variable and use a mean square error loss to dictate when firing should occur:\n",
        "\n",
        "$$MSE = \\sum^n_{t=1}(t_{\\rm spk} - \\hat{t_{\\rm spk}}^2),$$\n",
        "\n",
        "where $t$ is the time step, and $n$ is the total number of steps. In such a case, a larger number of time steps are expected to improve performance as it will allow the flow of time to look more 'continuous'.\n",
        "\n",
        "Is there a preference between latency and rate codes? We briefly touched on this question in the context of data encoding, and the same arguments apply here. Latency codes are desirable because they only rely on a single spike to convey all necessary information. Rate coding spreads out information across many time steps, and there is much less information transfer within each spike. Therefore, latency codes are much more power efficient when running on neuromorphic hardware. On the other hand, the redundant spikes in rate codes makes them much more noise tolerant. "
      ],
      "metadata": {
        "id": "UYDvqr1oMHQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Backpropragation Through Time"
      ],
      "metadata": {
        "id": "9SBjiLAX-ZhX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing the gradient through an SNN is mostly the same as that of an RNN. The generalized backpropagation algorithm is applied to the unrolled computational graph. Working backward from the end of the sequence, the gradient flows from the loss to all descendents. Shown below are the various pathways of the gradient $\\nabla_W \\mathcal{L}$ from the parent ($\\mathcal{L}$: cross-entropy loss) to its leaf nodes ($W$). \n",
        "\n",
        "\n",
        "<center>\n",
        "<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial3/3_6_bptt.png?raw=true' width=\"800\">\n",
        "</center>\n",
        "\n",
        "The learnable parameter $W$ is shared across each time step. This means that multiple backprop paths exist between the loss and the same network parameter. To resolve this, all gradients $\\nabla_W \\mathcal{L}$ are simply summed together before applying a weight update.\n",
        "\n",
        "To find $\\nabla_W \\mathcal{L}$, the chain rule is applied to each pathway. \n",
        "\n",
        "**Shortest Pathway** \n",
        "\n",
        "Considering only the shortest pathway at $t=3$, where the superscript $^{<1>}$ indicates this is just one of many paths to be summed:\n",
        "\n",
        "$$\\nabla_W \\mathcal{L}^{<1>} = \\frac{\\partial{\\mathcal{L}}}{\\partial{p_i}} \\frac{\\partial{p_i}}{\\partial{U[3]}} \\frac{\\partial{U[3]}}{\\partial{Y[3]}} \\frac{\\partial{Y[3]}}{\\partial{W}}$$\n",
        "\n",
        "The first two terms can be analytically solved by taking the derivative of the cross-entropy loss and the softmax function. The third term must be decomposed into the following terms:\n",
        "\n",
        "$$ \\frac{\\partial{U[3]}}{\\partial{Y[3]}} = \\frac{\\partial{U[3]}}{\\partial{I[3]}} \\frac{\\partial{I[3]}}{\\partial{Y[3]}}$$\n",
        "\n",
        "Recall the recursive form of the Synaptic neuron model:\n",
        "\n",
        "\n",
        "$$I[t+1]=\\alpha I[t] + WX[t+1]$$\n",
        "\n",
        "$$U[t+1] = \\beta U[t] + I[t+1] - R[t+1]$$\n",
        "\n",
        "$WX=Y$ is directly added to $I$, which is directly added to $U$. Therefore, both partial derivative terms evaluate to 1:\n",
        "\n",
        "$$\\frac{\\partial{U[3]}}{\\partial{Y[3]}} = 1$$\n",
        "\n",
        "The final term $ \\frac{\\partial{Y[3]}}{\\partial{W}}$ evaluates to the input at that time step $X[3]$. \n",
        "\n",
        "**2nd Shortest Pathways**\n",
        "\n",
        "Consider the pathway that flows backwards one time step from $t=3$ to $t=2$ through $\\beta$:\n",
        "\n",
        "$$\\nabla_W \\mathcal{L}^{<2>} = \\frac{\\partial{\\mathcal{L}}}{\\partial{p_i}} \\frac{\\partial{p_i}}{\\partial{U[3]}} \\frac{\\partial{U[3]}}{\\partial{U[2]}} \n",
        "\\frac{\\partial{U[2]}}{\\partial{Y[2]}} \\frac{\\partial{Y[2]}}{\\partial{W}} $$\n",
        "\n",
        "Almost all terms are the same as the shortest pathway calculation, or at least evaluate to the same values. The only major difference is the third term, which signals the backwards flow through time: $U[3] \\rightarrow U[2]$. The derivative is simply $\\beta$. \n",
        "\n",
        "The parallel pathway flowing through $I[3] \\rightarrow I[2]$ follows the same method, but instead, $\\frac{\\partial{I[3]}}{\\partial{I[2]}} = \\alpha$. \n",
        "\n",
        "An interesting result arises: for each additional time step the graph flows through, the smaller that component of the gradient becomes. This is because each backwards path is recursively multiplied by either $\\alpha$ or $\\beta$, which gradually diminish the contribution of earlier states of the network to gradient.\n",
        "\n",
        "Luckily for you, all of this is automatically taken care of by PyTorch's autodifferentiation framework. Variations of backprop through time are also available within snnTorch, which will be demonstrated in future tutorials.\n"
      ],
      "metadata": {
        "id": "8UYEmLk4-e4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Non-differentiability of Spikes"
      ],
      "metadata": {
        "id": "c7nYrxNFLybB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above analysis only solved for parameter updates for the final layer. This was not an issue as we used membrane potential $U$ to calculate the loss, which is a continuous function. If we backpropagate to earlier layers, we need to take the derivative of spikes, i.e., a non-differentiable, non-continuous function.\n",
        "\n",
        "Let's open up the computational graph of the Synaptic neuron model to identify exactly where this problem occurs.\n",
        "\n",
        "<center>\n",
        "<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial3/3_7_stein_bptt.png?raw=true' width=\"800\">\n",
        "</center>\n",
        "\n",
        "Backpropagating through the shortest path gives:\n",
        "$$\\frac{\\partial{S[3]}}{\\partial{Y[2]}} = \\frac{\\partial{S[3]}}{\\partial{U[2]}} \\frac{\\partial{U[2]}}{\\partial{I[2]}}\\frac{\\partial{I[2]}}{\\partial{Y[2]}}$$\n",
        "\n",
        "The final two terms evaluate to 1 for the same reasons described above. But the first term is non-differentiable. Recall how $S=1$ only for $U>U_{\\rm thr}$, i.e., a shifted form of the Heaviside step function. The analytical derivative evaluates to 0 everywhere, except at $U_{\\rm thr}: \\frac{\\partial{S[t]}}{\\partial{U[t-1]}} \\rightarrow \\infty$. This is the result generated by PyTorch's default autodifferentiation framework, and will zero out the gradient thus immobilizing the network's ability to learn:\n",
        "\n",
        "$$W := W - \\eta \\nabla_W \\mathcal{L} $$\n",
        "\n",
        "where $\\nabla_W \\mathcal{L} \\rightarrow 0$. "
      ],
      "metadata": {
        "id": "yXWeOeqZ5S_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How do we overcome this issue? Several approaches have been taken and yielded great results. Smooth approximations of the Heaviside function have been used, taking gradients of the continuous function instead. Friedemann Zenke's extensive work on surrogate gradients is among the most rigorous on this topic, and is [very well documented here](https://github.com/fzenke/spytorch). The option to use surrogate gradients is available in snnTorch as well, and can be called from the `snntorch.surrogate` library. More details are available [here](https://snntorch.readthedocs.io/en/latest/snntorch.surrogate.html).\n",
        "\n",
        "snnTorch takes a wholly different approach that is simple, yet effective. "
      ],
      "metadata": {
        "id": "ktpVgJisQU03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3.1 A Time-Evolution Approach to the Spiking Derivative"
      ],
      "metadata": {
        "id": "iBHz22CdgE0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What follows is a simple, intuitive description behind the approach taken. A rigorous mathematical treatment will be made available separately. \n",
        "\n",
        "The analytical derivative of $S$ with respect to $U$ neglects two features of spiking neurons:\n",
        "\n",
        "* the discrete time representation of SNNs \n",
        "* spike-induced reset and refractory periods of neurons\n",
        "\n",
        "**Discrete Time Representation**\n",
        "\n",
        "Given that SNNs (and more generally, RNNs) operate in discrete time, we can approximate the derivative to be the relative change across 1 time step:\n",
        "\n",
        "$$\\frac{\\partial S}{\\partial U} \\rightarrow \\frac{\\Delta S}{\\Delta U}$$\n",
        "\n",
        "Intuitively, the time derivative cannot be calculated by letting $\\Delta t \\rightarrow 0$, but rather, it must approach the smallest possible value $\\Delta t \\rightarrow 1$. It therefore follows that the derivative of a time-varying pair of functions must be treated similarly.\n",
        "\n",
        "**Spike-induced Reset**\n",
        "\n",
        "Next, the occurrence of a spike necessarily incurs a membrane potential reset. So when the spike mechanism switches off: $S: 1 \\rightarrow 0$, the membrane potential resets by subtraction of the threshold, which is set to one by default: $\\Delta U = U_{\\rm thr} \\rightarrow -1$:\n",
        "\n",
        "$$\\frac{\\Delta S}{\\Delta U} = \\frac{-1}{-1} = 1$$\n",
        "\n",
        "This situation is illustrated below:\n",
        "\n",
        "<center>\n",
        "<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial3/3_8_timevarying.png?raw=true' width=\"550\">\n",
        "</center>\n",
        "\n",
        "If instead there is no spike, then $\\Delta S = 0$ for a finite change in $U$. Formally:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\frac{\\partial S}{\\partial U} \\approx \\Theta(U - U_{\\rm thr}) =     \n",
        "    \\begin{cases}\n",
        "      1  & \\text{if $S$ = $1$}\\\\\n",
        "      0 & \\text{if $S$ = $0$}\n",
        "    \\end{cases}  \n",
        "\\end{equation}\n",
        "\n",
        "This is simply the Heaviside step function shifted about the membrane threshold, $U_{\\rm thr} = \\theta$.\n",
        "\n",
        "<center>\n",
        "<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial3/3_9_spike_grad.png?raw=true' width=\"550\">\n",
        "</center>\n",
        "\n",
        "What this suggests is that learning only takes place when neurons fire. This is generally not a concern, as a large enough network will have sufficient spiking to enable a gradient to flow through the computational graph. Armed with the knowledge that weight updates only take place when neurons fire, this approach echoes a rudimentary form of Hebbian learning.\n",
        "\n",
        "Importantly, the situation is more nuanced than what has been described above. But this should be sufficient to give you the big picture intuition. As a matter of interest, the Heaviside gradient takes a similar approach to how the gradient flows through a max-pooling unit, and also evaluates to the same derivative as a shifted ReLU activation. "
      ],
      "metadata": {
        "id": "r8W_hVKkf4HM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Training on Static MNIST"
      ],
      "metadata": {
        "id": "uDFwKmN9en4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Time for training! Let's first define a couple of functions to print out test/train accuracy."
      ],
      "metadata": {
        "id": "6D-fhT3Q7nXM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "def print_batch_accuracy(data, targets, train=False):\r\n",
        "    output, _ = net(data.view(batch_size, -1))\r\n",
        "    _, idx = output.sum(dim=0).max(1)\r\n",
        "    acc = np.mean((targets == idx).detach().cpu().numpy())\r\n",
        "\r\n",
        "    if train:\r\n",
        "        print(f\"Train Set Accuracy: {acc}\")\r\n",
        "    else:\r\n",
        "        print(f\"Test Set Accuracy: {acc}\")\r\n",
        "\r\n",
        "def train_printer():\r\n",
        "    print(f\"Epoch {epoch}, Minibatch {minibatch_counter}\")\r\n",
        "    print(f\"Train Set Loss: {loss_hist[counter]}\")\r\n",
        "    print(f\"Test Set Loss: {test_loss_hist[counter]}\")\r\n",
        "    print_batch_accuracy(data_it, targets_it, train=True)\r\n",
        "    print_batch_accuracy(testdata_it, testtargets_it, train=False)\r\n",
        "    print(\"\\n\")"
      ],
      "outputs": [],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "-IxcnBAxpkoy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Optimizer"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "OxfhunW6pkoz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will apply a softmax to the output of our network, and calculate the loss using the negative log-likelihood."
      ],
      "metadata": {
        "id": "T7ULsfh9bHr1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "optimizer = torch.optim.Adam(net.parameters(), lr=2e-4, betas=(0.9, 0.999))\r\n",
        "log_softmax_fn = nn.LogSoftmax(dim=-1)\r\n",
        "loss_fn = nn.NLLLoss()"
      ],
      "outputs": [],
      "metadata": {
        "id": "iqdVyjCNtdlp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Training Loop"
      ],
      "metadata": {
        "id": "GiqAVKzVbfPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We assume some working knowledge of PyTorch. The training loop is fairly standard, with the only exceptions being the following.\n",
        "\n",
        "**Inputs**\n",
        "\n",
        "The for-loop that iterates through each time step during the forward pass has already been nested within `net`. This means that the following line of code:\n",
        "\n",
        "`spk_rec, mem_rec = net(data_it.view(batch_size, -1))`\n",
        "\n",
        "passes the same sample at each step. That is why we refer to it as static MNIST.\n",
        "\n",
        "\n",
        "**Targets**\n",
        "\n",
        "The losses generated at each time steps are summed together in the for-loop that contains:\n",
        "\n",
        "`loss_val += loss_fn(log_p_y[step], targets_it)`\n",
        "\n",
        "Also note how `targets_it` is not indexed, because the same value is used as the target for each step. '1' is applied as the target for the correct class for all of time, and '0' is applied as the target for all other classes.\n",
        "\n",
        "Let's train this across 3 epochs to keep things quick."
      ],
      "metadata": {
        "id": "yGDs_dF2e1Sx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "loss_hist = []\r\n",
        "test_loss_hist = []\r\n",
        "counter = 0\r\n",
        "\r\n",
        "# Outer training loop\r\n",
        "for epoch in range(3):\r\n",
        "    minibatch_counter = 0\r\n",
        "    train_batch = iter(train_loader)\r\n",
        "\r\n",
        "    # Minibatch training loop\r\n",
        "    for data_it, targets_it in train_batch:\r\n",
        "        data_it = data_it.to(device)\r\n",
        "        targets_it = targets_it.to(device)\r\n",
        "\r\n",
        "        spk_rec, mem_rec = net(data_it.view(batch_size, -1))\r\n",
        "        log_p_y = log_softmax_fn(mem_rec)\r\n",
        "        loss_val = torch.zeros((1), dtype=dtype, device=device)\r\n",
        "\r\n",
        "        # Sum loss over time steps: BPTT\r\n",
        "        for step in range(num_steps):\r\n",
        "          loss_val += loss_fn(log_p_y[step], targets_it)\r\n",
        "\r\n",
        "        # Gradient calculation\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss_val.backward()\r\n",
        "\r\n",
        "        # Weight Update\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # Store loss history for future plotting\r\n",
        "        loss_hist.append(loss_val.item())\r\n",
        "\r\n",
        "        # Test set\r\n",
        "        test_data = itertools.cycle(test_loader)\r\n",
        "        testdata_it, testtargets_it = next(test_data)\r\n",
        "        testdata_it = testdata_it.to(device)\r\n",
        "        testtargets_it = testtargets_it.to(device)\r\n",
        "\r\n",
        "        # Test set forward pass\r\n",
        "        test_spk, test_mem = net(testdata_it.view(batch_size, -1))\r\n",
        "\r\n",
        "        # Test set loss\r\n",
        "        log_p_ytest = log_softmax_fn(test_mem)\r\n",
        "        log_p_ytest = log_p_ytest.sum(dim=0)\r\n",
        "        loss_val_test = loss_fn(log_p_ytest, testtargets_it)\r\n",
        "        test_loss_hist.append(loss_val_test.item())\r\n",
        "\r\n",
        "        # Print test/train loss/accuracy\r\n",
        "        if counter % 50 == 0:\r\n",
        "            train_printer()\r\n",
        "        minibatch_counter += 1\r\n",
        "        counter += 1\r\n",
        "\r\n",
        "loss_hist_true_grad = loss_hist\r\n",
        "test_loss_hist_true_grad = test_loss_hist"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Minibatch 0\n",
            "Train Set Loss: 72.08113098144531\n",
            "Test Set Loss: 59.14543914794922\n",
            "Train Set Accuracy: 0.2890625\n",
            "Test Set Accuracy: 0.203125\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 50\n",
            "Train Set Loss: 14.509231567382812\n",
            "Test Set Loss: 18.13492774963379\n",
            "Train Set Accuracy: 0.90625\n",
            "Test Set Accuracy: 0.8515625\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 100\n",
            "Train Set Loss: 14.841381072998047\n",
            "Test Set Loss: 13.378573417663574\n",
            "Train Set Accuracy: 0.921875\n",
            "Test Set Accuracy: 0.9453125\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 150\n",
            "Train Set Loss: 13.393373489379883\n",
            "Test Set Loss: 12.330821990966797\n",
            "Train Set Accuracy: 0.90625\n",
            "Test Set Accuracy: 0.9140625\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 200\n",
            "Train Set Loss: 11.487451553344727\n",
            "Test Set Loss: 14.469902992248535\n",
            "Train Set Accuracy: 0.9609375\n",
            "Test Set Accuracy: 0.8984375\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 250\n",
            "Train Set Loss: 13.681583404541016\n",
            "Test Set Loss: 10.47976303100586\n",
            "Train Set Accuracy: 0.90625\n",
            "Test Set Accuracy: 0.9453125\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 300\n",
            "Train Set Loss: 10.912232398986816\n",
            "Test Set Loss: 11.182395935058594\n",
            "Train Set Accuracy: 0.9140625\n",
            "Test Set Accuracy: 0.8984375\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 350\n",
            "Train Set Loss: 14.549544334411621\n",
            "Test Set Loss: 10.333059310913086\n",
            "Train Set Accuracy: 0.8984375\n",
            "Test Set Accuracy: 0.8828125\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 400\n",
            "Train Set Loss: 11.02597713470459\n",
            "Test Set Loss: 9.984179496765137\n",
            "Train Set Accuracy: 0.9453125\n",
            "Test Set Accuracy: 0.9453125\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 450\n",
            "Train Set Loss: 17.882648468017578\n",
            "Test Set Loss: 9.065406799316406\n",
            "Train Set Accuracy: 0.9140625\n",
            "Test Set Accuracy: 0.9453125\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 32\n",
            "Train Set Loss: 12.810504913330078\n",
            "Test Set Loss: 13.069978713989258\n",
            "Train Set Accuracy: 0.9296875\n",
            "Test Set Accuracy: 0.921875\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 82\n",
            "Train Set Loss: 12.2172269821167\n",
            "Test Set Loss: 11.74819278717041\n",
            "Train Set Accuracy: 0.9453125\n",
            "Test Set Accuracy: 0.953125\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 132\n",
            "Train Set Loss: 12.334637641906738\n",
            "Test Set Loss: 9.457991600036621\n",
            "Train Set Accuracy: 0.9609375\n",
            "Test Set Accuracy: 0.9453125\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-12-6b96b93768a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Minibatch training loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdata_it\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets_it\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mdata_it\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_it\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mtargets_it\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets_it\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "metadata": {
        "id": "LMZMxEV8dcTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If this was your first time training an SNN, then congratulations!"
      ],
      "metadata": {
        "id": "Taf6WZLojHTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Results\n",
        "## 6.1 Plot Training/Test Loss"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "HxU7P7xFpko3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "# Plot Loss\r\n",
        "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\r\n",
        "plt.plot(loss_hist)\r\n",
        "plt.plot(test_loss_hist)\r\n",
        "plt.legend([\"Train Loss\", \"Test Loss\"])\r\n",
        "plt.xlabel(\"Iteration\")\r\n",
        "plt.ylabel(\"Loss\")\r\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAE9CAYAAADaqWzvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAB9lUlEQVR4nO3dd3hUZdrH8e+ZmWTSe4MECCX0TgA7TawIotgLKIq919Xd1bWyrrqWd10WK7p2VFBYFQRRQRBClR5KgBRCSM8k08/7xznTUiBAJhG4P9fFRTIzZ+aZk2TmN/fTFFVVVYQQQgghRNAZ2roBQgghhBAnCwleQgghhBCtRIKXEEIIIUQrkeAlhBBCCNFKJHgJIYQQQrQSCV5CCCGEEK3E1NYNaI6kpCQyMzPbuhlCCCGEEIeVl5fHwYMHG73uuAhemZmZ5OTktHUzhBBCCCEOKzs7u8nrpKtRCCGEEKKVSPASQgghhGglEryEEEIIIVrJcTHGSwghhBDHzuFwkJ+fj9VqbeumnBDCwsLIyMggJCSk2cdI8BJCCCFOEvn5+URHR5OZmYmiKG3dnOOaqqqUlpaSn59P586dm32cdDUKIYQQJwmr1UpiYqKErhagKAqJiYlHXD2U4CWEEEKcRCR0tZyjOZcSvIQQQgjRKkpLSxk4cCADBw4kLS2N9PR07/d2u/2Qx+bk5HD33Xcf0eNlZmY2uZBpW5ExXkIIIYRoFYmJiaxbtw6AJ598kqioKB588EHv9U6nE5Op8WiSnZ19yIVJjxdS8QJW7ynn85x9bd0MIYQQ4qQzZcoUbr31VoYPH87DDz/MypUrOfXUUxk0aBCnnXYa27ZtA2DJkiWMGzcO0ELbjTfeyMiRI+nSpQuvvfZasx8vLy+P0aNH079/f8aMGcPevXsB+Pzzz+nbty8DBgzgrLPOAmDTpk0MGzaMgQMH0r9/f3Jzc4/5+UrFC5i3oZDZq/O5LLtDWzdFCCGEOOnk5+fz66+/YjQaqaqq4pdffsFkMvHDDz/w2GOP8cUXXzQ4ZuvWrfz4449UV1fTo0cPbrvttmYt63DXXXcxefJkJk+ezDvvvMPdd9/NnDlzeOqpp/j+++9JT0+noqICgBkzZnDPPfdwzTXXYLfbcblcx/xcJXgBJoOC06W2dTOEEEKIVvO3bzaxubCqRe+zd/sYnriozxEfd9lll2E0GgGorKxk8uTJ5ObmoigKDoej0WMuvPBCzGYzZrOZlJQUiouLycjIOOxjLV++nC+//BKA6667jocffhiA008/nSlTpnD55ZdzySWXAHDqqafy7LPPkp+fzyWXXEJWVtYRP7f6pKsRMBoMuNwSvIQQQoi2EBkZ6f36L3/5C6NGjWLjxo188803TS7XYDabvV8bjUacTucxtWHGjBk888wz7Nu3jyFDhlBaWsrVV1/N119/TXh4OBdccAGLFy8+pscAqXgBEGJUcLrdbd0MIYQQotUcTWWqNVRWVpKeng7Ae++91+L3f9ppp/HJJ59w3XXX8eGHH3LmmWcCsHPnToYPH87w4cP59ttv2bdvH5WVlXTp0oW7776bvXv3smHDBkaPHn1Mjy8VL8BoUHCr4JaqlxBCCNGmHn74Yf70pz8xaNCgY65iAfTv35+MjAwyMjK4//77ef3113n33Xfp378/H3zwAa+++ioADz30EP369aNv376cdtppDBgwgM8++4y+ffsycOBANm7cyPXXX3/M7VFUVQ1K2ti2bRtXXHGF9/tdu3bx1FNPcf3113PFFVeQl5dHZmYmn332GfHx8Ye8r+zsbHJycoLRTAD+b3EuLy7YTu6z5xNilCwqhBDixLRlyxZ69erV1s04oTR2Tg+VW4KWMnr06MG6detYt24dq1evJiIigokTJzJ9+nTGjBlDbm4uY8aMYfr06cFqQrMZDdppkHFeQgghhAimVinvLFq0iK5du9KpUyfmzp3L5MmTAZg8eTJz5sxpjSYcksmgLfnvcMk4LyGEEEIET6sMrv/kk0+46qqrACguLqZdu3YApKWlUVxc3OgxM2fOZObMmQCUlJQEtX1GPXhJxUsIIYQQwRT0ipfdbufrr7/msssua3CdoihNbjA5bdo0cnJyyMnJITk5OahtDDFqbXBK8BJCCCFEEAU9eH377bcMHjyY1NRUAFJTUykqKgKgqKiIlJSUYDfhsGSMlxBCCCFaQ9CD18cff+ztZgQYP348s2bNAmDWrFlMmDAh2E04LM8YL6l4CSGEECKYghq8LBYLCxcu9C69D/Doo4+ycOFCsrKy+OGHH3j00UeD2YRm8Y7xkm2DhBBCiKApLS1l4MCBDBw4kLS0NNLT073f2+32wx6/ZMkSfv3110ave++997jzzjtbusktLqiD6yMjIyktLQ24LDExkUWLFgXzYY+YSR/j5ZDV64UQQoigSUxMZN26dQA8+eSTREVF8eCDDzb7+CVLlhAVFcVpp50WpBYGn6wWCphkjJcQQgjRJlavXs2IESMYMmQI5557rncc+GuvvUbv3r3p378/V155JXl5ecyYMYN//vOfDBw4kF9++aVZ9//yyy/Tt29f+vbtyyuvvAJoPXIXXnghAwYMoG/fvnz66aeA1ivnecwjCYRHQvZqxNfV6JSuRiGEEKLVqKrKXXfdxdy5c0lOTubTTz/l8ccf55133mH69Ons3r0bs9lMRUUFcXFx3HrrrUdUJVu9ejXvvvsuv/32G6qqMnz4cEaMGMGuXbto37498+fPB7T9IUtLS/nqq6/YunUriqJQUVERlOcswQvf4HqpeAkhhDhpfPso7P+9Ze8zrR+c3/wdaWw2Gxs3bmTs2LEAuFwu71qf/fv355prruHiiy/m4osvPqrmLF26lIkTJxIZGQnAJZdcwi+//MJ5553HAw88wCOPPMK4ceM488wzcTqdhIWFMXXqVMaNG8e4ceOO6jEPR7oaAaN3HS8Z4yWEEEK0FlVV6dOnj3eLwd9//50FCxYAMH/+fO644w7WrFnD0KFDW2TDbI/u3buzZs0a+vXrx5///GeeeuopTCYTK1euZNKkScybN4/zzjuvxR7Pn1S8kOUkhBBCnISOoDIVLGazmZKSEpYvX86pp56Kw+Fg+/bt9OrVi3379jFq1CjOOOMMPvnkE2pqaoiOjqaqqqrZ93/mmWcyZcoUHn30UVRV5auvvuKDDz6gsLCQhIQErr32WuLi4njrrbeoqamhtraWCy64gNNPP50uXboE5TlL8ELGeAkhhBBtwWAwMHv2bO6++24qKytxOp3ce++9dO/enWuvvZbKykpUVeXuu+8mLi6Oiy66iEmTJjF37lxef/11zjzzzID7e++99wL2gF6xYgVTpkxh2LBhANx0000MGjSI77//noceegiDwUBISAj//ve/qa6uZsKECVitVlRV5eWXXw7Kc1ZUVf3Dp43s7GxycnKCdv+r8sq4bMZy/jt1OGdkJQXtcYQQQoi2tGXLFnr16tXWzTihNHZOD5VbZIwXfhUvGeMlhBBCiCCS4IXMahRCCCFE65DghX/FS4KXEEIIIYJHghe+letlcL0QQogT3XEwtPu4cTTnUoIXMsZLCCHEySEsLIzS0lIJXy1AVVVKS0sJCws7ouNkOQkgxChjvIQQQpz4MjIyyM/Pp6SkpK2bckIICwsjIyPjiI6R4IWM8RJCCHFyCAkJoXPnzm3djJOadDXiG+MlFS8hhBBCBJMEL6TiJYQQQojWIcELv70aXTK4XgghhBDBI8ELMMrgeiGEEEK0AgleQIhnHS8JXkIIIYQIIgle+MZ4ScVLCCGEEMEkwQv/MV4SvIQQQggRPBK8AINBQVFk5XohhBBCBJcEL53JoMgYLyGEEEIElQQvnclgkDFeQgghhAgqCV46k0GRMV5CCCGECCoJXjqjUcElY7yEEEIIEUQSvHQyxksIIYQQwSbBS2eUrkYhhBBCBJkEL53JYJCKlxBCCCGCKqjBq6KigkmTJtGzZ0969erF8uXLKSsrY+zYsWRlZTF27FjKy8uD2YRmM8kYLyGEEEIEWVCD1z333MN5553H1q1bWb9+Pb169WL69OmMGTOG3NxcxowZw/Tp04PZhGYzyhgvIYQQQgRZ0IJXZWUlP//8M1OnTgUgNDSUuLg45s6dy+TJkwGYPHkyc+bMCVYTjojJoMg6XkIIIYQIqqAFr927d5OcnMwNN9zAoEGDuOmmm7BYLBQXF9OuXTsA0tLSKC4uDlYTjohRxngJIYQQIsiCFrycTidr1qzhtttuY+3atURGRjboVlQUBUVRGj1+5syZZGdnk52dTUlJSbCa6aUtoCpjvIQQQggRPEELXhkZGWRkZDB8+HAAJk2axJo1a0hNTaWoqAiAoqIiUlJSGj1+2rRp5OTkkJOTQ3JycrCa6SVjvIQQQggRbEELXmlpaXTo0IFt27YBsGjRInr37s348eOZNWsWALNmzWLChAnBakLzrZ7Fg9UvyDpeQgghhAgqUzDv/PXXX+eaa67BbrfTpUsX3n33XdxuN5dffjlvv/02nTp14rPPPgtmE5qnZBvZthU4ZTkJIYQQQgRRUIPXwIEDycnJaXD5okWLgvmwR85kxqQ6sDkleAkhhBAieGTletCCFy7sdmdbt0QIIYQQJzAJXgDGUADcTmsbN0QIIYQQJzIJXgAmMwCq097GDRFCCCHEiUyCF3grXjhtbdsOIYQQQpzQJHgBmMK0/10SvIQQQggRPBK8wNvVKBUvIYQQQgSTBC/wdjUa3Hbcsnq9EEIIIYJEghd4K16hOLHLfo1CCCGECBIJXuCteIXiwOaQ4CWEEEKI4JDgBd7B9aGKE5vT1caNEUIIIcSJSoIXeLsazci2QUIIIYQIHgle4O1q1IKXVLyEEEIIERwSvMBvcL1UvIQQQggRPBK8wG9wvVOClxBCCCGCRoIX+CpeisxqFEIIIUTwSPAC36xGZFajEEIIIYJHghcEruMlXY1CCCGECBIJXhCwnIRdgpcQQgghgkSCF4DBhIqijfGS4CWEEEKIIJHgBaAoYDTLGC8hhBBCBJUELw+TWVtAVWY1CiGEECJIJHh5mMyyjpcQQgghgkqCl4cplFAZXC+EEEKIIJLgpVOMZsINslejEEIIIYJHgpeHyUyY4sIqY7yEEEIIESQSvDyMoYQbnNTanW3dEiGEEEKcoCR4eZjCCDM4sdilq1EIIYQQwSHByyM0kijFSq1NKl5CCCGECA4JXh5hsUSrFmokeAkhhBAiSEzBvPPMzEyio6MxGo2YTCZycnIoKyvjiiuuIC8vj8zMTD777DPi4+OD2YzmCY8jSrVQK12NQgghhAiSoFe8fvzxR9atW0dOTg4A06dPZ8yYMeTm5jJmzBimT58e7CY0T1gcEe4aLFZHW7dECCGEECeoVu9qnDt3LpMnTwZg8uTJzJkzp7Wb0LiwWEw4cdktbd0SIYQQQpygghq8FEXhnHPOYciQIcycOROA4uJi2rVrB0BaWhrFxcXBbELzhccBYLJXt207hBBCCHHCCuoYr6VLl5Kens6BAwcYO3YsPXv2DLheURQURWn02JkzZ3rDWklJSTCbqQmLBSDEUYmqqk22SwghhBDiaAW14pWeng5ASkoKEydOZOXKlaSmplJUVARAUVERKSkpjR47bdo0cnJyyMnJITk5OZjN1ITFARCtWmT1eiGEEEIERdCCl8Viobq62vv1ggUL6Nu3L+PHj2fWrFkAzJo1iwkTJgSrCUdGr3jFKLKkhBBCCCGCI2hdjcXFxUycOBEAp9PJ1VdfzXnnncfQoUO5/PLLefvtt+nUqROfffZZsJpwZPQxXrFY9G2DzG3aHCGEEEKceIIWvLp06cL69esbXJ6YmMiiRYuC9bBHT+9qjFFqsdhkLS8hhBBCtDxZud5D72qMxYJFNsoWQgghRBBI8PIwGHGZIolWarHIGC8hhBBCBIEELz+qKQwzDulqFEIIIURQSPDyZzITilMfXC+EEEII0bIkePkzmTErdmxOWcdLCCGEEC1PgpcfRa94SfASQgghRDBI8PKj6GO8bE4Z4yWEEEKIlifBy48SYiYUBzbZMkgIIYQQQSDBy49iMhNucEhXoxBCCCGCQoKXP6MZs+LE6pCuRiGEEEK0PAle/kxmwpCKlxBCCCGCQ4KXP5NW8ZLB9UIIIYQIBgle/oxmfVajVLyEEEII0fIkePkzmQmRWY1CCCGECBIJXv68C6hKV6MQQgghWp4EL38mM6GqXSpeQgghhAgKCV7+jGZMOFiZV0rvv37X1q0RQgghxAlGgpc/kxkDKiG4qLW7cLvVtm6REEIIIU4gErz8mcwAhOIAwO6SLkchhBBCtBwJXv5MYQCY9eAly0oIIYQQoiVJ8PJnDAX8Kl4SvIQQQgjRgiR4+dO7Gs2KdDUKIYQQouVJ8PLnHePlBKTiJYQQQoiWJcHLn1GveGEHJHgJIYQQomVJ8PInFS8hhBBCBFGzgpfFYsHt1kLI9u3b+frrr3E4HEFtWJtoMMZLtg4SQgghRMtpVvA666yzsFqtFBQUcM455/DBBx8wZcqUIDetDchyEkIIIYQIomYFL1VViYiI4Msvv+T222/n888/Z9OmTcFuW+urt5yEBC8hhBBCtKRmB6/ly5fz4YcfcuGFFwLgOhG74epVvGSMlxBCCCFaUrOC1yuvvMLzzz/PxIkT6dOnD7t27WLUqFHBblvrC40E4KZhSYAELyGEEEK0LFNzbjRixAhGjBgBgNvtJikpiddee61ZD+ByucjOziY9PZ158+axe/durrzySkpLSxkyZAgffPABoaGhR/8MWlJUCqDQKbQKkOAlhBBCiJbVrIrX1VdfTVVVFRaLhb59+9K7d2/+8Y9/NOsBXn31VXr16uX9/pFHHuG+++5jx44dxMfH8/bbbx9dy4PBGAJRKYTWFgOycr0QQgghWlazgtfmzZuJiYlhzpw5nH/++ezevZsPPvjgsMfl5+czf/58brrpJkAbK7Z48WImTZoEwOTJk5kzZ87Rtz4YotMwWfYDUvESQgghRMtqVvByOBw4HA7mzJnD+PHjCQkJQVGUwx5377338sILL2AwaA9TWlpKXFwcJpPWw5mRkUFBQcExND8IottjtOgVLwleQgghhGhBzQpet9xyC5mZmVgsFs466yz27NlDTEzMIY+ZN28eKSkpDBky5KgaNnPmTLKzs8nOzqakpOSo7uOoxLTDUFMEgM15As7cFEIIIUSbadbg+rvvvpu7777b+32nTp348ccfD3nMsmXL+Prrr/nf//6H1WqlqqqKe+65h4qKCpxOJyaTifz8fNLT0xs9ftq0aUybNg2A7Ozs5j6fYxfdDqW2lFAcUvESQgghRItqVsWrsrKS+++/31uBeuCBB7BYLIc85vnnnyc/P5+8vDw++eQTRo8ezYcffsioUaOYPXs2ALNmzWLChAnH/ixaUnQ7ANJNldhkcL0QQgghWlCzgteNN95IdHQ0n332GZ999hkxMTHccMMNR/WAf//733n55Zfp1q0bpaWlTJ069ajuJ2iiUgFob6ySipcQQgghWlSzuhp37tzJF1984f3+iSeeYODAgc1+kJEjRzJy5EgAunTpwsqVK4+oka0qMhGAZGONBC8hhBBCtKhmVbzCw8NZunSp9/tly5YRHh4etEa1qQht1fpkQ7UELyGEEEK0qGZVvGbMmMH1119PZWUlAPHx8cyaNSuoDWszkVrwSlSqOSBjvIQQQgjRgpoVvAYMGMD69eupqtK20omJieGVV16hf//+QW1cmwiNBFM4iYqM8RJCCCFEy2pWV6NHTEyMd/2ul19+OSgN+kOITCJBqcImwUsIIYQQLeiIgpc/VVVbsh1/LBGJxKlS8RJCCCFEyzrq4NWcLYOOWxGJxKmVEryEEEII0aIOOcYrOjq60YClqip1dXVBa1Sbi0wixr0Ri93Z1i0RQgghxAnkkMGrurq6tdrxxxKRRKxaydb91VTU2omLCG3rFgkhhBDiBHDUXY0ntIh4Qt1WjG47S7a14gbdQgghhDihSfBqTHg8ABlhNn7bXdbGjRFCCCHEiUKCV2PC4gDoGGGnyupo27YIIYQQ4oQhwasx4XEApIbUYbHJAHshhBBCtAwJXo0J07oaE421EryEEEII0WIkeDVGr3glGOqosbnati1CCCGEOGFI8GqMPrg+wWChxiZjvIQQQgjRMiR4NSYsFoA4pQaLVLyEEEII0UIkeDXGYARzDNFYqJExXkIIIYRoIRK8mhIeR7Rag93pxuGSPRuFEEIIcewkeDUlLI4Idw2AzGwUQgghRIuQ4NWU8DgiXFUAVFsleAkhhBDi2EnwakpYHGantkm4xS7BSwghhBDHToJXU8LjCXVoFS/pahRCCCFES5Dg1ZTwOEIclYAqi6gKIYQQokVI8GpKWBwGl50w7FLxEkIIIUSLkODVFH3boFgs1MjgeiGEEEK0AAleTdG3DYpVLJTV2tu4MUIIIYQ4EUjwakpYHADtQq3sr7S2bVuEEEIIcUKQ4NUUvasxM9JGYUVd27ZFCCGEECcECV5N0SteI5V1lFVWtGlThBBCCHFiCFrwslqtDBs2jAEDBtCnTx+eeOIJAHbv3s3w4cPp1q0bV1xxBXb7H3T8lD7Ga5Tlf1xU/kEbN0YIIYQQJ4KgBS+z2czixYtZv34969at47vvvmPFihU88sgj3HfffezYsYP4+HjefvvtYDXh2JhjvF9e5PoB9zvng1s2yxZCCCHE0Qta8FIUhaioKAAcDgcOhwNFUVi8eDGTJk0CYPLkycyZMydYTTg2Bt+pSVBqMOz9FWdtRdu1RwghhBDHvaCO8XK5XAwcOJCUlBTGjh1L165diYuLw2QyAZCRkUFBQUEwm3BsLnw54Nva6rI2aogQQgghTgRBDV5Go5F169aRn5/PypUr2bp1a7OPnTlzJtnZ2WRnZ1NSUhLEVh7C0KnQ6yLvt3XVFW3TDiGEEEKcEFplVmNcXByjRo1i+fLlVFRU4HRqK8Hn5+eTnp7e6DHTpk0jJyeHnJwckpOTW6OZjYtu7/3SZqlou3YIIYQQ4rgXtOBVUlJCRUUFAHV1dSxcuJBevXoxatQoZs+eDcCsWbOYMGFCsJrQMkLCvV/aJXgJIYQQ4hiYgnXHRUVFTJ48GZfLhdvt5vLLL2fcuHH07t2bK6+8kj//+c8MGjSIqVOnBqsJLcPlW+7CKcFLCCGEEMcgaMGrf//+rF27tsHlXbp0YeXKlcF62JbXdxKseAMAV11VGzdGCCGEEMczWbn+cDKGkH/7LgBc1so2bowQQgghjmcSvJohIiIKu2oEq1S8hBBCCHH0JHg1Q2SYiWoiUOwSvIQQQghx9CR4NYPZZMRCOAZbdVs3RQghhBDHMQlezWRRIjE6atq6GUIIIYQ4jknwaqY6QyQhjmpcbpWPV+7F4ZINs4UQQghxZIK2nMSJxmqIJLJ2P8//bwtvLd1NVZ2DW0Z0betmCSGEEOI4IhWvZrIYoonEwltLdwNQVGlt4xYJIYQQ4ngjwauZ9tSGEIdvjJfN6WrD1gghhBDieCTBq5lMkQlEKVZC0Db4Lq6yseOADLYXQgghRPNJ8Gqmy87sD0AsFgAWbz3A2S//xDfrC9uyWUIIIYQ4jkjwaqbI2CQAYpXAKtfnq/PbojlCCCGEOA5J8Gqu8HgA+ih7vN2NANVWR1u1SAghhBDHGQlezaUHr9dC/4/HTB96L66qk+AlhBBCiOaR4NVcevACOMWwGYC4iBCqrM6mjhBCCCGECCDBq7n8gleJGgdAz7ToI654udwqN763ilV5ZS3ZOiGEEEIcByR4NZc5xvtlFREADOgQh83pPqI1vQ5UW1m89QB3frSmxZsohBBCiD82CV7NZfCdqsxIJ3nTLyQjNgyA6iPoblRQAHCrLds8IYQQQvzxSfA6CtFqNXw+hfPW3AIc2QB7z+babkleQgghxElHgtcR+P26jSxwDaGTbTts+orkg78RiuOIBtjbnFrwcqkSvIQQQoiTjQSvIxAeHc8BfWC9R6ayn5+3lzS7gmV3SsVLCCGEOFlJ8DoC0WEmKokMuKybUsAbC39n9prmrWDv7WqU3CWEEEKcdCR4HYHY8BAshmjtm/aDALjIuJytYTcQt+e7Zt2H3Ru8JHkJIYQQJxsJXkcgLMTI1LMHat9Et8cd24HzjasASC9Z1qz78HQ1uqTkJYQQQpx0JHgdocSIEO2L8DiU+E7eyy2uJk7lS71gwZ+933qClxS8hBBCiJOPBK8jZQzV/k/oghLd3nuxxamdyg35FZRZ7FhsTvIOWqC6EH59na/W5nPWCz9idbi41riQFPVgW7ReCCGEEG3I1NYNOO70vwJcNhh0HSx+2nux3eFAVVWufvM3rjmlI+v3VZCz6wA7tDVWue/T9QDYSvfwTMi7rHH/AlzXBk9ACCGEEG1FgteRMpog+0bt6+h23otrKg/S+U//A6CkysaKXWXEU9vgcFt1KQAR2ILfViGEEEL8oUhX47HwC16xWACV/wt5lR4HvtWuVuoaHGKrLgeghnAAVuwqZVNhJdTKptlCCCHEiU6C17GI8Y3xilUsZCr7GWf8jVtKpxOGjRgsDQ5xW7SxXRZV64O8cuYKHn/9PXihM2z8slWaLYQQQoi2EbTgtW/fPkaNGkXv3r3p06cPr776KgBlZWWMHTuWrKwsxo4dS3l5ebCaEHz1Kl5nGDZ6vx9jWBtQ8TJjB8BQp3U1VusVL4B+hl3aF3m/NHiIH7ceYNGW4hZtthBCCCHaRtCCl8lk4qWXXmLz5s2sWLGCf/3rX2zevJnp06czZswYcnNzGTNmDNOnTw9WE4IvOg1CtJXsuxsKeCbkXWr0SlaqUk6M3xiveKoBMFm1LsVaNQyb0wWAAX1tCSXwx2F1uLjhvVVMnZUT1KchhBBCiNYRtODVrl07Bg8eDEB0dDS9evWioKCAuXPnMnnyZAAmT57MnDlzgtWE4DOGwK2/oA692XvR35zX41CNJCqVxCi+rsYPQ58jhhpC7VqFz6Co7K+0aneDtrbX3A3FWGy+Dbe/27i/NZ6FEEIIIVpJq4zxysvLY+3atQwfPpzi4mLatdO66NLS0igubrwbbebMmWRnZ5OdnU1JSUlrNPPoJHZFMWvbCL3rPJfPXSMpJ5oEqonG19XY1VDEhcbfiHBowSsUB4UVWvBS9OBVanGw40CN95h1+yoAaB8b1hrPRAghhBBBFvTgVVNTw6WXXsorr7xCTExMwHWKoqAoSqPHTZs2jZycHHJyckhOTg52M49NnRam1is9AShVY0hUqgIqXgB9lTxi3JUAmHFQWKEFM5MevFwYyC/3hbXKOod29w5XcNsvhBBCiFYR1HW8HA4Hl156Kddccw2XXHIJAKmpqRQVFdGuXTuKiopISUkJZhNax4iHITKZNb8NhUoHpWo0iUoV0WrgchLDDFtxYgS0ildRpXZ9hGL13mZfuW9cmCd41doleAkhhBAngqBVvFRVZerUqfTq1Yv777/fe/n48eOZNWsWALNmzWLChAnBakLriWkPox8nOS6KhMhQyoghgSqiqaVQTaCb9X1edFxGlqGADEXrNg3FSWGlle9CH+Ee01cAhGHn94JKKmu1wOUJXjanWzbVFkIIIU4AQat4LVu2jA8++IB+/foxcOBAAJ577jkeffRRLr/8ct5++206derEZ599FqwmtLpnLu5LaY2d3FkxdDYW09lQTIUaiRMTu1VtXJtniYlQxUFBeR09Dfu8x4crduZvKGJrURWLHhjpDV6gdTdGmWWjgUOyHISi9dBtTFu3RAghhGhU0N7JzzjjDFS18SrNokWLgvWwbapXuxicLje/qdHeyz53jQAgX0kNuK0ZB+vzKwIuC0frctxZYkFV1YDgVWt3tkrw2ldWS2Wdg77psUF/rBa3+l348Xl4fD+YQtu6NUIIIUQDsnJ9CzMZDUTpY7b+47yQZ53XAlDoF7wOKEmEKU4qah0Bx2bGaD+OSw0/c2D3RirrHCRHmwGwWqrBFXj7lra9uJozX/iRca8vpdxiD+pjBYW1ElQX2GsOf1shhBCiDUjwCoKfjacA8JHL1+VlNcZQo2iVsAJTB7or+9hlvibguM4x8NVN/XkpdAaxn12C3emmXWwYsdSQ8VZ/eDoZZo4CVYWDufBCFyjPa7F2bymq8n59sOY43MTbrk9MsFW3bTuEEEGjqioOl7utmyHEUZPgFQQ7zb3JtH7EHjXNe5nJqFAa2o5qNZxKYyKgLaLqL0S10UPJB0Cxa+EhLSaMHso+DM5aSOoOhWt4Z/EG9n3/KtSWwpZvmmzHkm0HyHx0PiXVzQtRe0t9Myor6oJbXQsKh95+qXgJccJ6dv4Wsh7/FrdMOBLHKQleQRBh1paMuHNUN24b2RXQuiDL4gewzt0Vh9L4+COTq46I8i0AlCoJALSLDaOzQVvB/qPqgQC888NaVm3drR0UFgu/z4ZdPzW4v9cX7wBgg2cs2e5fwKmFsEVbitm6vyrg9nn+wav2OAxensB1FBUvVVX56Le93hmlR6PrY//j2fmbj/p4IQRsLqziq7X5TV4/a3keAFanLLMjjk8SvILAMwj+1K6JnJWlLf4aajRQeOrfuN7xKGVNFKAURy0UbwKgxqXdR1psOJ2V/Tgx8XO1NjMyFgtx6CHDXgtfTIX3x2tdkNZK7/3V6et/FVbUQWU+zBoHG78AYOqsHM57JXBT7r1lFtrpq+RX1AZvjJfV4eLLNflNTr5o/KBKWP/poW/j7Wo88orXlqJqHvvqdx6avf6IjwUtuLncKm/+svuojhdCaC547Rfu+3R9k68PnkW362R9Q3GckuAVBJGhWmhSVYgI1apfJqNCz/axqBgotzdx2u21kL8KgBhVq0a1jwsjU9nPfmM7yvXZkrFKDQmKXtXRV80H2Dz7KZjeEaqL2XGgmp0lWgDZddAC1drWTGplPrV2336QnrESn67ay6q8cu9sxsomuhorau38/but2J1HP8bihe+2cf9n61m642DzD/ruMfhqGuSvbnjdzy/Czh/9uhqPvOLlWSdtb1ntYW7ZONsxnA8hREPVfvvW+vPsdSILS4vjlQSvIEiPDwdARcUcop3iEKOBzMRIAOxNreJRq61D5VBCSKAKUMmIDydT2c9uNY0KtOPHG35loGGndozlgPfwxB1fAmArL+Dsl3/G5nQTRzVdd/0X+4InAPh1/VbvHpEx1FD56e1greSRL34HYGhmPEaDQkWtA6vDxVPfbPaOEbvxvVWc+cKP/HvJTuZtKDzq8+NZnb/G2vgLa32frdpHbaW+X2dVAc/M28yvntCmqrD4afjgYr+uxiOveHm2ZTriQGmrhtXvUWM9Drtmj5DD5T6yKqUQx6C40tro5Qa94mWVrdTEcUqCVxA8cVFvHr+gF6d3TfJWUkwGBaNB4YGx3TlvQGbA7S+wPcdrzou1bxQjK5IvJ1RxEWuw0jkpinTlIDvsCVSqWvC60rTEe6zr4E7v14q1AoCte7TxEbeGzGdd2C1cW/4GoXu1bsVOpT/h+vYxAC43/kTS9o9h2auEhRgY2SOZyadlEhseQo+9H5O74E3eWbabr9cX4nar/LjtANV6WFqzt5ztxUc3e9BzThp7C1dVlQ+W53n3sayxOXn4iw38kKcFG2dNKW8t3c2834u0A2x+49TsRz+4vsbm2yXgiGyZB9/cg+1AbsPrDu4AS+kRt+WPqKTaRtbj3/Lf3/a2dVNEGyiqrGv13TOKqxofk2HQS16yh604XknwCoLosBBuPqsLBoNCp8RIQk0GHjynBwB3jcmie/vEgNtXEUGdqo2touOp1MZ2AyArykZ0iJtopY5SNYZKveIFsM7dBVd8F0r3+gZzpygVAGzYpoWxR0wfN2hbhnKQHrtnEYoDh155cx7cidXhZljnBMwmI3HhIQwo+pzQ9R9wp/ErNu/eR5XVgX+x478r9nLOP39u8hzkl9eyZm95wGW/51eysaDS+wL+686DrNxdRpXVwVPfbGbGTzvZ/8FNFM17jpcXbgfwBrBih3Z+LGUFAJR6lrvQu1ABX1fjUVS8PIGywafo32bCk7HeSQkN6CHPaqlqeN2Hk2DJc0fclj8iz8/h01USvFqaf9d/W7I5XY2u33egysqpzy/mxQXbWrU9xVWHrnhJV6M4XknwCrIos4ntz5zP2b39Vq43mQNuYzeEc3Gm/sbebTSmaG1AfntTDSG2CgAqiMJk9gWvd5wXYI/KIMVd0uAx9xXk0y3agaI2Xb2JwErnCO2FbeOmDQAkRmqzLWPCQ4h2VZBl28SDIZ8TvecHyo9wtt8/F+Zy86ycgMsu+r+ljHt9qTd4/XfFXv46dyO/7jjIO8t2M/3bLbTbNZuHQz6lvUkLMp43fCPac7GXadW8gzX6G0SNX/CyW/T/Aytx6/ZVsOPAocOYxaa9iDeoeH2vVQf9Jy0E0MOe1dJI9a+mGLXmADsOVDfdRec69Juuzelqu+698jw8advTAoezXlv2b4T5D4Bbxrgdjc9y9tH7r9+Td9DS1k3hplk5DHp6YYPLD+hDDX7a1vC1JhjMJu1tqbi68eCFVLzEcU6CV1swBi4nsfJvE+kx4irtm/5XYI7RQlqKsQZqywAoU6PJTIryHpOvJlFnimn07sPt5YxMOnQ3YJRipZNZCyO9lH2YcBIfobWruLyGeKXGu86Yoa7siLsV95XXUmqxU1nraDA2yOY3Dbygos47hiyFCu/lvUq+h5oS73i0MPRgWukJXvr3jQWvehWvh2ev54Xvth6yvb6uxnov5m49cDbVfal3bzqs9d44XQ5w1LI5r5CzX/6ZTYVVlFvsWPwHDB/MhacTYeWbPDF3I5P+/WvAXVTWOejx5+/4x/f1Kg15S5sOgkdDVWHxs1C0QTu/e1dogerVAbDiDQBvux31A1bu97DqLdC7uU9qdgv88tJhw7S/BZu039+j7bZvSb/kauMm6wd9z7dGTx9fEPkvjnrYMV5HW/E6mAtvnOZ9bRWitUnwagvGkMDvTWbofi48WQmxGSix2rIR7Qyl2iKpQDnRpMeFew/JV5OpNUTTmHilmqzoQy+aOqlvLKl616RZcZCmlHHKsqmwawnOmsDZhvFKNZsKG+lKo96LdOFaKNOWUyiq1CpVS7YfIOvxb/nQb2yQf/Ws2urUZl0Cmcp+7+XnF74OL3bjn19pY9MiFO35mGq0Qf0HPYvC1vgmF6BqL8Tueut4lVTbKD3MFkiegf4Ol4rr55fhfw8H3qCp7ku94mWrq3e9VTtfVosWkPaV1TLo6YVMnbXKd5v/PQhAxe/fMWv5HnL2BHbNvvKD1t36WY7fmkaWg/DehTDndt9lqgrbvz+iN/wAdgv8/AK8ez4sfQU+uQZKtTXg2LsC8HXFOl31Kl6OusD/T2aLn4VFT8Gmr3C51WZVKj1Z5o+0Fmj9qq9TD9uGVgheFrvLey72V1mptjoanEdPM466q/HnF+HAJtj27TG09ASlqrD8jRNmbOoflQSvtnCYPRfjUzpiUc2MMa7XqglAuRpNXIQvsJUQS7nq63rcYsjyfp2gVNMp7NDLItx7VjoJqu+N/gzDRmIKl8IXNzPz0syA2yZQzY4DvjDTISGc2PAQTjFspq54J5V1DpbvLIWZI+G1gdTYnOzXP61+v2k/Cm5mfbvUe3yZHoJ6KHvpo+xmQ74WTjoZ/KpXut6GPQCEox0TbtWCVoT9IM55D0FFwzFHluoK79cut0pFnYPy+uuSqSps+w4cWjtrbL4Xcde272HrfAIGtXkqXtZ6AVQPXk5bvfNt055TEpXMDf0zw+acRQhOVuzy+5RdpU0QKC7ztdd/VqVnC6eYcL9ZsJ7nW+JXwduxCD66HJb+k6PiqVbZa7SZtXXlvt9Rg/bYnoqX22kPfFHWK36qo7bxoKGq3iplffd+svaYx4z9klvCrpI/yE4F1fqED1S6PvY/7vx47WEP8VSR3G05W7RiLzwZy1kGbQ27+oHG6tB+J43Bz11U+S1js2ZvBf2eXMDnOYG/P56K11F3Neof0DAYj+74E1nBavj+T/DN3cF9HLsFvr4rYDmkk4kEr7bQ1EBtXe/0WIxJ3cgsWwqb5wBQrkYFfOI0GY1srPIFr6xh53q/TqCKdia96yu2Y+MPYqsmwl7KJncnAMYY9DeJyCQGJQYGw3ilmtxi35vbGd2SefycTD4JfQbzh+P595KdXP2mr5vs0be+waFXRrbtr+Z8w0r+x12konebWuxEUcv35keZb37cu7J+J6UYh2qkQPVNPuipaG/M4XpXY5irGhNO7jLNwZQzE377d4OnVldT6X2OlXXapIAGK/Gvegs+vgLWfQhoXY2x1AAq7ur9WhdmrS9gWKortC6Kv2cGriWmBw9X/a5GqydMHmCAYReJrgPEUU1kqO/FXtVDW7uazdxhnIOZwK5IT5WpoLzOF2oq92n/m/26mT1LiniqVEfK/8XPs9G4Z7aoJ3jpA8AfcsyAf3TxVdf05/Dn2Tk8Na+RVftXvAH/7AMHArt6HS43c9YVepcxaY7Hv/qdP325IeCy695eyeiXGu7a0FqcLjdVnqVE9HNR59bO2fwNRU0d5uUJEa09YzDAfu1nMNm4ACCwOxzfhBNPW4PJf/1AzxCEH7YEfiA75gVU3Z7g1cSyPicz7wfMFhzK0JjCdbDmfdj7W3Af5w9KgldbcOpjF7KnwkO7Gr1JWFr3gO8riKKk2sYDif/m1tDnGN45kb/lDwTAERqHacRD0P9K1rq7ka4cJEmpAqMZEjID7/jCl7X/bdVEO8v43d0ZgLFGPUyEhGvdWX4SlSpy/QanJ0SGkFmlDZxXLCXsOFBDAr6KWMdCrYSv4KagpIxehr2EKC6yDAXe21xjXOT92uFy0y0likxlP/lqEhWqbyxbX8NuHrugJ+GKL6zGU00VEY2eNwBHbRWlu1bjer4juzet1M5frT1wb7dfXvLcGACTpZj1YdO4PWQ+Sk2xNrarwBewqqvKtSqT6oIDfgFDP95lt5BAFaE4cLrcDStjQLxSg8Xu8u4KoOqhLUap5aGQz5ge8iaWujrKLHaenb/ZGxZtTjcVW5doXYBl+u9LWKzvjhU9zLkb72pcvLX40FWhugrf154X3Dq9Mqd3i9fob8bnqfpuB55gpj//kjJ9AoPdAruWaPdjq6F8ozZYu7Jwe8BD7ik9dEXWX2FFHX+du5EPf9vLxyv3NX3D8jwoOrqdB47Wn778nf5PLtB+t/QxhoWlFU3e3uFyB2xG7/kw1RIDxQ/W2Jq9L2sAkzZjOF5flNlSb5alp22t0dXoqXglRfkmINUfW+bJf0d9zjx/J8qRv/3tLKnxLkz9R5R30MK5//zZNwb2SHkq3fWHw7Q0z3ug8+QcoiDBqy1EpWj/p/WFyMTGbxMSGCxshNI1OQpTu75UJg1hTK8UrJjpY32bvZcvhPA4uOQ/bKcjnQ3FRK5+AyKTIFJ7rBnOi7jX/BR0Ha3dYdlOFLeDK8edj131K7lXFTUIXkmGwBea+IhQMkq0N+C6+O7sLKkhTfFVTc4x5vBX0/vsCLuOrWE3cJdpDhA4hquT39cpVNAtOYpuSiE71HSsRt/YtTMj9jLtzC5EKr6uwn7xDpz4tbnzWd4vK9UIzNaD5K7/DSNuZn/7PaCNofFUkKgu9nUL6dUeY502Y+vW0O8wu/UXg59f9N5vbU2Vt2tQrSr0hTg9eKzaXsCasFv5JPRp7Q2hkU+M7c3a/W4urGLJtgPgqMWl+t5UJhqXEfXDI3y3cT9v/rKbgoo62utbOIV9/xBsnec3LsUvRHq6TFTfG1FFrR1VVbE6XNz63zW8+UvjAd//HAC+dnt+B/T79oyBc6qGwNt5g2ct1XUOePsceH8ClleGwj+6UlOpBbjcooqAh/R0XZua8Wb+wGfreX/5ngaXN6gSvToA/nNWg9u1BFVVWby1WAvVfr5aq32YcMwcA3n6eMSNHzLVOJ8h0WXgDOzifnHBNs5/9RcWbNrP7oMWb/ddbROrtB+Jh2dv4MHPjyJ46j/DOGr4u2kmSYseDLjaU1kaYlsFM8487FCJY1Gl/56lx4V5L6sf+Dw/g6PvatR/hk18UGlg/gOwW1s6Z8xLPzGmDSusAXb8AJUFARdtKapiW3E1u492lqynN6beBLAW5wleJ+nYUAlebWHAVXDVJzDkhqZvk5QV8O3Xd57O/WO787cJfXh7SjYXDWgPgIVwEttnem932pTncBn1QfgRiRDfCcLjSZn4PLfecAOE6tWkrf/T/u90KmqIb9A+Nfv9xqpoPNsTZYZbOcWUS1ZSGDFV+kw7WzV5pRYu76H9Ki10DWGgYSc3mr7DWG+J1ImdfJ/CkhTfp/7/hP6TQREH6aIHL3eoL3jF2Qrh4HYiFDtVJm3j8GEpKrH4XlgKMy/2fr1F7USC6yDuUm0ts2i7bwp8ZWmR1sXoVxWxVh6gzGJH0YNEjMtvDFb+Sn42jwD07stqbWD/srW/0+Ux/fzpLxxRivb/YMMO6qz2wIVddUNTFJ4wzeI/n3zJlHdXYnDWsUvVfo7VCX352dWPkJLfA6pTvdppXYoV4XqX8T69NO9fpXLpb+62aijfw5q95Qx8aiHfbtzPmj3l2J3uQ2967j8j0dO9WqXvTGDQPvl6up9cnsBbVwZz74B9WkVRddSRWrsdijcCEGktBqeVDjXaud57UH8MlwO2zCN3v/Y7lRJthop9sPLNJpvX1LIC/m+8R9xV53ZB3rJm3/yHLQe48b2cBntxpsaEYcCNeb+vOtquPIe7TV/xheNO+PahgNuv3VMBwL0fLOP8F7+nW90GjLiwtMCaVPvKar1LP3jU2V10/tN85q4raOIovJW6BKWaK0xLSMoN3BPVc56nVr4G+zdAiW+Wrc3poroFd23wdDW2i/W9JhnrdXF6xpwdfVdjYDf5Ibmc2mvGrIuO7rGC6b+Xwj97B1zk2Tj8qFf193Q1tlbFqzk/gxOQBK+2oCjQ43xfzbwxp94FNy7wfts/I45Qk4GwECMRoSaSoszk/Pls3p0ylLgI36eTDl16YRz9mO9xTr8Xpv7AJUM60DMtBsx68CrIgYgkSO2H2an/sQ28Vvs0mJ8T8IknXq3kOuMCvlIe4BPTE5xV/iVmi/bGrNSVo6owJF77Q/rMNaLJp9Q9VKuiZCgldA6rZa9bW69skGEH5+/5O6GKi1x3OqrejbbO3VU7cNv/iFBsFKJV7/rHO4hRtD9Yt6pw0Y9p1IanAVCd2B+DoqLs1caceSpxFxhWED9/qvbpNecd7RSoify0dguDn17Y5CDPn6POB8BmqfRWvKz6WmIOl9v7ppXqV/Ezrnvf15XpZ1h0CTeYvud1x18xo73B5KtJANS0P4NSYjDYKlHyV2FCe3PokRbNWEMOtY56wcK/ouZZRmPHD/Bqf1bMnwVoSxT8ulMLUk3tvQkEhjjPedDDd4m+Xppn8oHT85JxMBfW/te7nIdZtRNp85thWk9RiX6/uQvg02uoy1/HFvMU/mr/J3x8lTbDs6bx4yvrhUZPtdF/LNIB/3DWnIHqP/0d3rug2WNM8vVtrjyzdT2So80B3ewesfrvJzt/DLg80qwF1xkh/2SJ+X7u3HM3txvntsgiqmUWe4P7OVBtRVXh8a82snZv47/j6LOA45TGqySeN/GDJu1vjKJ13uumvpdDvycXNDjmw9/2BHSpNpeniywzyTd+1VPwKq2xYXW4vEu+HPMYL0dgoHe63IEVTbc7+GOdjpbb77nbfT83Tyj1/H/EPDPCDUEOXp5z72j8Q1WwVNY6AoectBEJXn9URhN0HA7XfQVXf97oTZKizIzqmdLwinYDtP+rCiEsBpK6+a4zhfnGBHUZAQYDROmLu/aeoP2/Z6kWDP08HfIe8W7thVv5bQYhFu2NOcJVxeMhH9HZUIyqGHju3lu8x7gVIzvd7bzfh1fvobuyj6Xme8hybCGpxyne69qHaOEvV83AFKEFr63uDtQl9IJdPxGOjR32eAA6hNWSYKxjs7sTWY6PKLXCHYlvcbX9MRzdxwEwXNkCQJpSRhS1vBH6GtH7teoM27+lKjyDve5UEvTKm6Om8enTu8L6YFNDqKgow1GhhU1PyMovr6OkvEK7DF+lLHHJo76xWH46uLTAFqPUEYH2gvOTewAHxr1H+SkPU6lGEl6zj8f33817IX8HoLuyjzdDX6brwcXanZjCcaQOoLriIAeqGv/UeNP+pxmsbMetarsDAL4B4B7rP9W6XB11jYZOW5k2lmrxxn38nl/ZsOJVHtj1F46NMId2P7vcabhUhUp8lcuqCv386uHKWbmfcMWujRnzTA7wm8ywfGcp7yzVqksV9UKjp8vYP3h51ntr7Hw0yjN+T3/ue0otjPjHj94Fe0ELMp5KkWeJhbCQwJlwIUaFFKWJQAOBA7hrSvhL4e10VIoZaNjp/VCQZSjwLuB7tNxulfJae4OB8Z5219icTHzj18bHgNkP3S3lmeXoXb5m7h2w5RsA70b3/sHe5nTx+FcbmfhG0xVFVVV5e6nWne6vuMpKlNlEaoxvjJdb1X5/hzzzA5f/Z7l3uYnmdDUu2lIc8DPVHlw/rt74oqveXBG4gOxn12kTSfza/Ifh/zu+a4n3S09IPuqKl6dSfwJ2Ne6vtDLgqQWHHnbRSiR4/dF1HQ3dzzmyY9L6af839qlFUXxjgpJ7af/fvBhu+1ULeh59JsL4/4ML61VuRj7mnVm3xd1BO9w4j8jNn6JEpZKckgYmrZvgwLBHmOXytd1YkccAz+beQERCBgzTgpqpVOu+KDBlYIyIA6CSSNS0AVC8iTDVRoE7AbeqkKRUk2iopYoIxvRMYUBGLJuKbfzq7ktIshYyPYu/pilldFAarri9LbQ3pUR7qxVxNP7mk1flpoYwCotLKNy3S79P7Q3zo9/2eFesb6dowWuu67QG92EhDLtqJNbi+4P3LI9Ri5mwvuOICg8PmDBwhnETGcoBUpx+3b7p2fBYASuNg4lw1/DKwq1sLKj07VEJOJN64cTIRcbl5JfXsl5fqsNWa/FVgqqL4atp8NpAeDaNvF0NF5cNqdUrWYqdHSXV3sH1Dj14bdoUOJYoTHEQ5dIe6+/OK3nJeRmF7njv9WaXRatU6EFHqfWNI1Q9HwT8FsO96s0V3lmS9bsR69Z9Rs7ir7BY/YOX3wt4vSqFqqrMXVcQuDiu5wVf/1v4LGcfe0pr+WSlb3mLuz5ewz2frKOosg6bXkHwrKruUW11HjJ4uf3/BgvX0MWRyxjDGm/FFsCqhh6y4rWn1MIbS3b43vjd7gbVwYo6B27VNwnCo/736/dVNHyAw+xt6gk4EW6/Cta8++G9cbRH+znm+i0Am1+undtDVV0KK608PW8zt36wOuDy4iorKTFmUlzFtEML4l+vL+Scl7UxVp6lZ+Dw63ip715A3UfXc9HrSwOv8HY1+sK6qqqsyiv3jQMFbUyln+oWGIfXYvwDi9/fjednddTj37yTglowZDptgRU6aJOuRs+kiEVbm67MtxYJXieiiAQ4/x9w7ReNX+8ZE5SgzWgkNgNS+2gz5a7+DDoMh25jYfB1MPQmnI/5TefO9o1L20Km7/K6MkjUK2sh2sDYuLTOfOU6k9y4M+CM+1BcNkYa1vmOiUyCC16AOG38Up0SQWh4DIbwOAAq1UjCMvqD5QAhOLCo4VQqUYQ7yok11FKpRmpdPZGh3rEtMfEp2Ey+SkuqUk5GI8HrC8fpRMSl0tVQxKL2/2GYZ0enXhehdjyVt7mYu+x3sqvEgkUN41rTIjqpWsUqWankT6YPOWX7C3QwaPfdwaS9+c5ynsPySWsCHks1hFBJFGHlvo20O+jrrFlVM9FmExFmo3cTdI9uSiEJNr81jMLjwWCkwh2OUVH5elWutgWT3xunJSqTMqKJUWpZuLkYl1tlQDLMqZvseyMpz9P+9yx/kL+R2hBfSAIw6C+8YTjYtr+GGpuT9rFhuPSJGDXFOwNuH4adBKUaOyZ+NAznDdfF1OAbIB2t1GpVET14RVp9kyvcilYVclQVk/nofD72Cz/1Z2d1UIpJW3A72T9PYd2aFd7LA/b1K9kGrw2G4k2A9sZ9zyfreG9Znu823n09tcDgmUXnvzHzrhItjFdbnd6xM0aDwncb9/PD5mLvdZ49Ui1q4FZg2nPzq5Dpa7ANMgQu+xGm2OlXMg9Wva3dpt7uAFPeXcUL323zLQL809/hxayAfUrLLFq7rQ53QFCtXwFb12jwOnTFy9OlF+mqgrA4/Y4PQN4vXBqlLUWxzS947dVnrIY0svBXZa2DnLwy756Q9ceHFVfZSI0O48LF57I87C7v5fsb2bfxcFUdZc8yxhlXNFw82fNBxa/itasZg9GrqnzBs8EOF/Ut/5fv7ywY/AOL3wcNT9i1HXXFS/85HkslqnyP734AnkmBj64IvI13VmPrdTV6ftdiwtp+GREJXieq4dMgtfehb+MJXv66nwtTF/jGggGm0DAY+7T2L8rXtbnb1DXw2PTB+gH6TLzEjjx1xamYr/scuowE4ELjSt/tI7UxXp4Xc1toPEM6xaPqa1RFxSVhSOvjvXktZmqMcWA5SDS1VKkRevDyveG1i4ugtLNvIGx7QwXPDmo4TmN2eVfCYrXn0rXsJ0ZF7ITodnDFf1Fu/I6xd/2bb9xa9cqOr2ph73UJALeY5jOm8ivv5aFu7QWkgihqDL5zBxARHk5kXDKK3wv9BckHvc9JURSizCaqCAxe7ZVS4i2+wdyqWQuUJU6tohiLhXCs5Bf6qmKVhhiq1XBilDqqrU5MBoXxHWxEYkUtWKfdqN6is92UAg6Y2jc4RwBm7GwvrsZic9IxMcL7ObiDEvipMQwbCVRTqsYwNFObBGFRfQOko6nVlgrQg1eayxe86lzaG3RZsRYyX1rgW3piTV5gF3ASvjc/tdx3bvwX3lQ3z4WynWz/7304XG7W7q0A6q0O73lj0d8gPG9Y/mPFPGvRldbYvfdf53Bx639Xc9P72nIq1VaHd6urQbaZ7K93HvdW2L3dcI5yrVJ8RljgAP0UpYLril+A+ffDa4Pg98ChBZ5dGrxjmjbpv3eeSkfBGqpKChigaIHOfzmI+l2Ya/c1Up2zNRyjxoEt2oLIP7/oDTjRriroczH0u9x7s4F6iNy+33cfe0q1EBNpbvgG98Dn65k0Y7m3+lBrd/HN+kLv9fsrraTF+gJ7TBOVaDhMVedQYdJzncOqPfcnYyn78f+8VztcjVfqakp8fzeeWb44rA0eS60pge8fw/HuRQ2XN2mpPU3t/sHLLxB6uhqPZMzgnl996/J5fheOJRC92h8+mKh97anS7qi3B6jjyCtedfZm7FtbV6GtD9bI7TwBPDosyOPXmkGC18ksocvhb+Nx+t3aP/D2/59x5ujA27TXg1dyT+3/6DQmDsqgY2JE448VqQ0sR69wxSW141/XDCYqVltiY1ivLpDa13tzK6HUhLWDg7lEuGuoIpKkKDNJUb7xCKmxZkzj/sGzjqv5vMOfURQDKZve9l7/oXouN3b4H07VQHKo3yfh/FVaRUnXMTGCjHgtOKQrWki6034Xy/pP5377rU2epgo1Sus2um4O3yROBcBgMhMRkxRwuyHhWvCoQ2u72WSgmsDAlq6UEFXj656sM2jBLM+ivXDEKTV8Gfoknfb4KpulaizVRJBm1p5bx4QIMowVADjL91BaY6Os0Fd5AwhRXJQ4wnCrDSsU6cZylMK1DK/9iREhWwjXl/VorwTucxem2ElQqihTo73BqwZf8IpS6gIqXh0NvuBWY9XuMz9fGzfmWak/mlqyvx7Dc6a3UPRN0mMVX3VPrfS9YdfW+t78tu7QzllS1SaW7yz1VjPCQvxe7uyBFS9PONpX7gvHLv1NsrzW7t35oNyvelJtdVBjc5JmKKdcjcJOiHczd+9zszn5SN8uy3YwD4AEx35UfOc62W+PUtxO2PsrHNzhfdO260HA2w3m6SqzVWl7ar45isGfD2eu+a9EUofxu4e94bp+xWvngYaBZGdBMe76bwU7FmlbgC1+GqfNAqhEq1UQkYjVnOC9WU+HNpZyt9+6bHn61wrasiH/+Wmntyt1X5l2nWfD7QPVNu76eC37ymp5acE2LBUHOL/2a+99nWJoZFFencXm5EC1lRveXUlp/XWrAqpN9d6EPRViRy1Ua3+HvXP/4716VV5Zo12/tlLfGnLeLtyZI+C5wLC9p1C7z5CqvdryJgf1CufOH7E/k87QPzc+ZvdIuOyNV7zqHC5uMs7n5h+HNBi4/uPWA6yuty0Z+1ZpW4X9pI0p9a3P51fxqixo2FXYFE9wy9e3RmuqG/tIxnit+DfW75+k11+/Y8ZPjYzPylsGq9/Tvv72YW1FfM/j+/FUsyP8FrFuKxK8Tmbh8Ye/TWNu+QUueJHh/foEXu6peF36Nkx4A+IzfdfFpPu+rj/2zNN9oVfAunTIACC7V1ctnOlLYNSqZg4mnwolWwhzB3Y1AkSGGjGbjKTERjHprhe46Pr7YdI7gU85LJzFuRUAuHpqA/E9Y9Lqn4/kaK2S5gkbm9RMZq8pYLua0eSpqTFEsXV/NXQdxa9mfbyXMcR7326jdp8d0KoVdXr3lKIo2EJ8q9HvdSeToRwkrMIXkipcYTz4+Xq2VWovHNcbF9LLEFi9+i7PSa0SQbxBe0HrmBhBkqpVjdzle3lq3mYWLG04k2+rNZ5CGq4p1529vG1/iGecL3HbnvtIVhqf5RWudzWWqdEM8wQv1b9yUYt561fYd2kDrv0rZpHOCgDKirWB7Kkmre03m+aRYC/katNiJkZq3Yb+Y/E8+3YmGCzYLRXey43l2htdglLDT+u2sfug9uIfEEL0NwRrdRmlldXe4JV30OKdRel0q4BKuw1vkKVPbvi9wFdd+G1XGW4VMs3VHFDjADCogW/YUdR5A5y73Pezqo3qSKWqjenzdFV6Fa6D/xviXZPM4Q1eelXPE7xqy6AwsFv7FtM3RK57B+Zra3H5B4g+7WMoqbHhXv+Zd4ZuaY2NPUUHyHfX+9lX+CZPRNYWEEOtFiojEinHt3hvurqfBKp8Ez2AvXq4qqhz8PriHTz/7VamvLOKVXllpOiD5sO2fsFgZTspaEFg6/5qXl+8g0uNP3POHt+40sH1umU9EiNDOVhj599LdrJn+3r2zH5MqyYVb9bezP0mt0RTFziTzRMGnFbvhI4Qly/IPPTmPB74JKfBYzqqfFVab/DybN3lcsKGz+HFHmzfFVjRpEC/r+JNhLprSXEdoKx4HzwZC7n1KkHNVGfxq1L6LV1jdbi4xviD9k298PHUvM3e/V/LLXZtLT3PxBZPZU4PTrkFB7WFnmvLtCUrFv61ybaUVNt8W535dX+zbxXMvbPxgw4TvKwOF9Pez2FTYSV89yhhy/8JqPy6/OeGN37vAvjmHu1rz8bntaUNbub5Ha2/F2lbkOB1MuoyUltK4mil9IRhN2trhPUcB5fN0lbEj9UG2xOZCIOuCTzGf1+0O36D3hd7ux89FS/vYrKdToOLXoVMfTFMfbZllFKHo4uvylZFREDw8p9x1iMtWvveMzvTHANDbmB5+mTvbZJ7j9Q2Jj9NH0tSbyun2PDAgJinpjL/9yJ2qo13yxEazandUvn3kp1c/85KdrlS+Tl8DFz+vnfCg8EUhqoYiK3TPj2P6NPJe7jDL3gVksRpho0Ybb6gs/aAi9mr87GnDKBWNXOFaUmDJhTYIqlwhxOham8k5xlW0nuntt+nufA3Bue93ehkg1w1nXvtt7PW3Y2DxtQG1x9OGHYSqKZCiWFgxzggsOKVrhyk7/L7CbWX69/7Xhij9TXQxtp/4L8hz/Jx+ZVcb/yeW4zzWeLSZuh2N2lvep7V1avVcCKs+4nBwprQm7mowLdPZVfFVwnbue1372DvGpsLdv8Cu37yVt7Clr+E6eXu3tmfTrfKu79qb5xOl8ptxm8YlPsa11bMAHz7Z5qxoyx9Sduey1hJsaqH9nqVgWilltKKKrBWYqz2raXljEjlQvvzvOqc6H3+XvrYNA9PZqi2OtlxoAa3J3jVlUFR4BZKZ3u2/tJn7nmWAfn2njO5cmgHkt0HMXx1s7a358InqPrhRSIVK0X1Q7dfxWhY5Xf0N+hBJjyBA26ty9tTJRto2BEwW9Iz3k5V8S5nsjKvjMtmLOe33CI6Kft5Tn2NL81PsjLsDiYYlnrPf5YSuN5YXL2lOk41bMKMnYyECEotNsosdp40zWJw3lvabOx/n6o9N7/g9UHoc4FVXm9XYx0fLNJCkWeoQBg2Fpof5vTdvq5Hj827fOMta6yBAfvuN2ZTtHUF1OynNK9e96LnZ6TvBhGvVFO8WZ/xuXImqtq8DdX9BQSvemO8dng+GOoL+nocrLF599Ad/6+lnP3yzw0Xk9W7Le22Wn7bXeZbz8+/+7t0pxYaC9fhdLkZ+uwPPDxbf87+a0C+fbZ3y7sAq96CzXO1r5sIXr8XVLJgc3HAxIjLjD/xge1eNi2d2/S6dKH6BCVbw0qbZ5zgUS9D0oIkeJ2Mrp8LDx3lvn7+jCFw5YfauI+hUw+9LhnABS9q/xK7wuWzIFQf0+SpeHnCoMEIQ6ZoS2oAnPMMe1LPZoErm+TOA72D8ZOSU+meGu0dGF1/qr+3jXes1GZtXvQKPTpneq/yBDb6Xab9fzCwCy5cv7/vB/8bLnyZ164aQnaneNymCKrNaQ0fKzyey7O18Pnz9hI2FFqYlfonLXQNm6bdRlFQIlOgTHtzv+f8Ad7DrX6TAgrUJJI9i8zqoW1VkZMos4l3bx3N8tBTASg2BC4nUkM4cfFJmN3am8uVux8nvM73SX2y9QP6G3Z5qy0euWoGjoxTmGh/CmLacaSuM/1ApqEYQ1g0EaH6/o5+waur4fD7FoI2mxPgqZBZ2DHxkOMWrMZIMgzaG3icYsGtKmxXM0jDN2N1aK3vk7BR8b2JRdYVeod7zPhpJ8waB++Px7/7KVapJb+0msEd4zindyqvLsrlvyv2UOdwcba+lZbDrxs2xKjwf2EzGFP4Hy4yLidJraCEOKBhxSuGOq7fegtM70i4zS/wRiaTryaz191IyHX7DTi3VhGDhfGGZVQW5nL2yz9xsFL72X760zr2bA6sXno2lXcZzPy0vYTPV+/DoEDPtGhSYsIY4AlQ+zfAslfovO4FIrFS7TceD4DyPJxGrWJ5YfXn/Df0ee3yiETy7drf7RZDFk7VwDuhL3Kd7SOW5h7k/k/XcbCylvXmm7nF+A0l1TamndWFrBStav2A6TN+Mt8f8FC3mb6hbNuvZCn5nBEXWKmIVSyEYSNDKaGDUszHoc8yyfgzHRMiUFVtplod+hjP9Z9o/+/+meoC30zdgYZdsOI/sPFLfQN4vbt46zyu2/O436OpdFQOEKHYmKj+0ODHUlDsq9LW31ZJLfqdjdu01w/H/i2BB+br41r1Kkw81ewvq/Be/fSnP3Pd2ysb7IpwKHW1WvAqUWMoKzvInR9plU+rw4URPVjk+UKL3emm2uqkSA9eoeU7OMewClet3vWoB3VVr56FYafMYvd2xWKr1sb9AWydr/2//mNvdXPOukIu/89ySvc33GHC40CVlTlrC7S1FD3jE5sIXrv1iS3+hcpzDNrf4m/rfucf329reJDLgdOzePiXN8H7EwKu9nQ1tsSaecdKgtfJqhU2vG1g2M3av/o81TC/Af0BIhJwXPoeQwf0o3u7aLh9BUz4F3fe8QBRZhPx3opXE7/OyT0gTgtElwz2dRN6NtsluTucfk/Dbkl9LEB52hkwdCoXDWjP7NtOY+PfzqVs8F0scA0JfJzwOC7s3463J2cD2niLKM8MmqhkuOYLmDIPotPApVcI/HYN2FzuC47hGf1895uqfW0llCuHdiAmLISPQi/jLef5bEm/LKAJD17Qj7P6dtEXxW38U3S0Usc7zvN5zXmx97Id7nRuPD2Tv1/aj7Cz/9Tocc3hjPFtyu7f1Xg4q9zdA7euAta6u1FCHM6odNqpWmgZlOTGGRqDJbw97ZRSOprKGrs7XKn9AV+XZgwWrjQubvLxs5QCYsNDeOXKgWSlRPHvJTsx4qKPkgdAilrqfUNrH2NmDFrgSVHKiXIc9HY1GtXAT9NmxUFnu/aGbPD7eRiitcBVSkzA7euod86qCrnZNJ/XQv/FuGWX8KTpPW/XZHXZfpItgR8WPBZuLWHyOyupLsmnXagVRVFIjQmjn2FXg9vGYKG2/uOW72Gjo5HKbmQieVYttG93JGFStLBwr+lLrnt7OV+uLaBX3SpiFQt/CvmYy4xLGNwhhgX3nUWU2URPpeFemz0N+3i15kEWmh8mvc73hrrbnUosFu4xfclS8z1MM2pv+N2UAjro4y+3F9cQjd5N+Pts77HFm37CGhLn/T5p41sw+wb40DcxoL6/RH7t3dYsksBA4FQNRPstAVJtdVJcZaVKD6y9DHuJdWu/i5nues/R84HOE7yUaioO6oEmdwF3bLmGZTsO8N6veU22LcD+34nZ9F8ADqjxlJQcYN6GIpwuN3UOl3enEW9Qcjmp3TifgcoOamxOqqwOFoY+zMzQf1JXqX8Y0Cu1bj2ImRUHBeV13t06cNTCG6doAczzwcBgCtjDd+XuMmYvadhF6/Hg7A3c++m6wAv9Jhy53Kq23yuw82ANfZTdrDDf4b0+S9Eqjhabk3KLnbyDlsDZpbbqgKEA/uubgW/T9cMtQ9IaJHiJtucpcx9iteRuKdG8ftUgzCajVikbdK03tHi6BAdkxB32oRIiQ3nyot68dNmAwCvGPgVZZwdc5Kl42et9Eg0xGkgZdRu3OO7jvdjbYcwT2hX6Ppgjuif72p3sFyazztaqV9F+1TK/4HXNGT29X59345O4u4zW1lMzaZ/ozQY3U07PBGC3ksEzzutIzdAmLThVA+XtR9J3+FgM4bGYcHKpIbCroVT1VdR+U3vxfth13u9LiKVLUhRXDO1IZO9zGWydwY8R59GYZa4+jV4+jzMo73ej93v/rkbwrdLfmIL+d/H3gQuwRPq6XrepWlg2J3ViiHU5U43zOa29kdDoRGrDUmmnlNHJ1PgaWsakrthD47xLiTxq+pjpIW8F3Mbl9/J3nnElsWEmIkJNDM1MoKCijiylgDDFwTJXH0yKmzR9kdwuUTZviOqn7MaoOn3BSw9nRZ3GUxzTj/rqFO2chMRqvwOlamDw2kC3gNmttrJ9nGHYSKkazWZXR6aYfKvEDzDsJMIvIJT7hbhYVXsD+zL0SZYpN8Le30iNcHO6IbAbE7Ru4AYh2WWjVI1tcFviO5Nboz2HEjWOvzuu9F61yTyVK4w/crHRt3DqP0JmchobUBSFM8J2Bpzzxigu3+QFa0xnEo11nKq3+TqTVoUamVztXTja7nR7J794P8yghbMt4YMb3L+rYG2Tjz3V9al3TJmj3oeAGsKJptY7MPuh2RuYvTofs77DREflAImq9rvYJ6Qw4FisFeByoFo8Fa8anJW+CnCiUk2aycLa+kt92Gpg4RPUVJUHjKFjxhnE7dd25ihW42mvlNJT2UuNzYnN4SYOPQzVlWkVvtXvEjfnWt4O/QegzRz1rHPoKNGDuKMWassw2rXgEo5dWxuvql6luro4YBbsjgOBXXqm2mIaZQpvfPkPv4rXjJ92cvbLP5FbXM2uEgujovYG7AGcadDu22SrwGJ3MfLFJTz5pd8YR1sVhvqD+fXxlaqqUlmnr50owUsIfFOZj3J/sM5Jkbx3w1Cendjwja4xU07vzKVDmh4g7+EJXo2NCQgPNfL1nWcx8Zan4Mz74a/lMPZvAJiMvj+r607t1OBY704BACG+N9nHx/XRFqid8j8MIWYM130Jl73nXf383lGdyYjXqg13jNLWTMvM1INXdDrx0+ZqQS5MewN+KXSG975/an8TQ2z/YY87BTcKv7s70zEhnKKrFjPF/hCgkKDPDlUUhW8evZgz+9Rre4Q2DmiO+/RGz9eIcZO5Vg+PL1zan8tO0dd16zmOS0P/wwP22/TnHNHg2IvHjuEvE7PZceXP/O7OBPBOYghJ0Nrxl5APMVrLIDwea2QGZsVBf8Mu3H4bpnvX0gqLxRWTwbWmRVxlXERqWMPuBVe4b1zTvaYvuS3/QXBY6d0uir+Z3mWyUdtg/Vv3MAAu6aq9iHc2+wb4e9bk8ozxqtRD077TniN1jG8dKg9bmBY+Q/XgVeYXhgHyXfHsDvHtNFFXsJH+yk4+dI1hov1vLPULvUMN2kBpz0LGG80DvdclKFXEUuNdZ461H5Dy61MN1hADrWu2QcULLRRu1e8btHF1hMezocJMJZFsVzP4t2s836VrzzNCsfH3kDcZZghckDfmwCpYPYsZtj8x2riOgvDu7Olcb10n0MJrTLpWfR50Lb26ZJJsqg0YDwjQWdlP+7hwIqljhfkOOvnNkPW3xNEr4Ht3VCpGVa/W1Psd/K9zDADDDVuoVCNY7B4UcH21GkGUUkdmovbztTvdvPr9RsyKdn/JSgUJagUAia6DNGA5iNviq3iF1AWOsxyZ5qC40i9cbfwCvpgKy15hzlvPMOy5RY12RR5Q44hW6vjO/CjVFaVYnVrFq9Sof/gr2w2l2s88UakmkrqAxYaNBzZ624e+PEuhIY1wg5P8Cr+Kl049uN1XwbOUkFtcjQE34w3LCMNGCv4VaL9eFbeTjPhwQqm3g4bfchLL9fGA24qr2VlSQ8+oxpeaMFh9j7Flh18F11ZNJPWO0cecWR1u7/IwJ3RX44033khKSgp9+/qWAygrK2Ps2LFkZWUxduxYyssPsdWGOHn00dd86TrmqO9iZI8Ub9dgS5l2VhfOzErisuwOjV7fLyOW2Ag9LBoC/5Tm3XUGn047JWAfTS//ilf9sDnyEcjUg42nK3ToVAiJJH7wxd6bXTokg7zpFxIer3UHhbn9XnDM9SoVE2cyYpo2UyxH7cGBqN5YCKdjQgRhGf1Yor/JJEb62poeF44ptN64H32B3H9cNwomzoTsGwOujm7X1Rs6Lx/agcGd9AqXyUxtZDq/qT15LepeuPKjhuckVpv1mpUahUlfkmHwoGHMu+sMbWNtnVK6E8ITCEnU1qDr79pEtdkXZJ926lW80ChCTFpgfcL0PqGhgYublqlRmPRNAHf0up0ZzovoYVkNeUsZHFbEZNNCrjL9SFF4N5a6tdewUTXfsiD0IbJM2ifvXe4078D4q8YM5aIB7bna/jhPOCYTHhnjDcABTzNRGz9niE4hJswU0NVYkDqS951j2UCW97LQjZ9gVFSWubQPFW+4tHErNlX7vXGoRpa7tTCWHzfMe1ySUkV3vWumTg2FXUsw7vuNZcogXnVqf2+1kb7f66p6i/cCjMnuwwT707zg0EJSDeHc8fFadlW4eK7HbL5wnQmAKblbwHHtlTKq4n2v+2z7Dhb8xfttRUQmnXoNbfB4y9pPgfs3a9XnCf+C8DhinSUkK5V87jzLd8PyPSSHGzjFsNlbEfHfmsxjSUUq0+z3sfzM9/nEOZKtna71XRkROJngE9coQAvSe9RU3nGezyp3d+/1tYYIYqijU6IvsPl3R2YoJcQr9aot/iwlqHpXY8cwKzHOwDDZxVylbQa/51d4cwzMvhG2fwfA5ZXvMif0z9imd8O65pOA4w7oYwsByP0eh91GjFJLbqhePS/bBZW+SQGdlGLvOC+AyHK9O9Jy0DvutCIqizDVSu7eApwVgcFL+eQq70LMak0xOwpKeNb0Nq+F/ovLjD+R5r/MTKjf75Tbgdtpb9CF67/khUlfcDe3uIb88jrSTY3v9Rnt9lXcXNW+0P3Nym046+odo08S8d/S6oQeXD9lyhS+++67gMumT5/OmDFjyM3NZcyYMUyfPj1YDy+OJx2Ha7MLU3oe/ratKCUmjA+mDvcNwj8CfdNjGd6l4fIMgG+Ff2jeWLuUXvB4oXdSQQDPgrb+s3jqv+FH+4LJY46pbD/vA0Bb4yvKbxXnhpMT/MaIGUJ8s1atlTDgCm0m65T/+W4Tlxl4uKK/vKhufe89hV9jzoOMbN9tTrtbm/ygiwg18bBjGj+4BtFj8Jn0TY+FgX4zZCv3Qng8Z5+uhYwMDmCNSOOg3mX3xCN/xnHNHDjrQYzna68vdUoEiWrgWLBCJRVFn91m734hbzkv0K4ozaVrna87LvH8x5n9yOWgGBlcuYDuhgL6WrVBvmtV38/xzEF9GZoZT57ajlmuc7XNsBN9FT8AolJ94wrN0ZzaNRErvkBY0u8W1qvd+Hv1eYyzPcMBNY6I8q0cVGPIUbUQYO9wJr2s77DQrXWj7VDTKVS13zNXUg/vpIl4pYYeBm2s0duu87Vtvg5sxpXchwN6dS6ik68rbp+azM7kwK72xJT22Ahlhz6L16kamb9BqyCM6tcFVX/7iG7XnfpqOo3R9pgd8Sgc2AR+s3PtYcne4LPL7fsQMuG0et3/YXHeddF+CRupXWYIAdVFaE0+Zxg2em+6R/X9jk+2P8JnzhFsUjP5QR1K5+xzedQ5jV8r4gBwo2A3Bn6oKFITA+7rN7UXl9mf9F7mDo0mWqklNcrIlcbFdFaKiNJDd6ka3aAq5+GtqtYcwGjVQuKZrhWMNq4LuF0HUznFVTbUhU/4lp/QhSouBhp2EekoJX/ZpwHXDenqG4dn2jIHl0V7jG1G/WdSthsq9mIxa68Tmcp+CsrrtDAOGN1696zDgvrzi7hVhZqYbhhwszZkKiW7N0DWOTwV4zsXHpaCrbxWeRdXmbSN4Dsr++nov7CyyaxtOxettdFRW0WkUm9hVr+uxtKKKv5hmsGonNuwO93ertv64vwCbpLf8jZLVqwizFFvuRt9WRRP8EqKCqX2aFf1b0FBC15nnXUWCQkJAZfNnTuXyZO16fyTJ09mzpw5wXp4If64el/ccvcVHg/hCdrWSx7mesEryvfmZiOU3pkZ3DMmi4sHpRNiPMRLgP8CjJFJcPYT0O1s6Hmhdpmi+KpzoG1V5U/fwYCQCB6/oBeRoUZG9kgBs1/32jlPa5Mf/PyuduEmx0NER+qfmDsOhxt9Y5uISMCcmOn91hqexkT735gVcyvhMfGEZI2C8HiUTqcyI+R64qmknUMLIZ7lKQ4aU1EcWvDqkdWTCacPwB0aAwufIPT7B1FN4ah3ria0/yUkxcUEbBqfYdHe8HPcfu2OSiPKb6X2KLMJkrK0LugrP9Rm7sZ20LbmAohM4dw+gTNjY+M8C89G0GfICNamaJWptcZ+3s3JL8/uwKCu6RRE9qZaDec/znHsUNtjV40o8Z04yzCLzxLvwICbK1LyqVbD+dDpC1RnnjGCK0brlbF2vqCzV01hzbCX4E9+W1TFtOefVwxg3OnaJBKHX5fuIH3JEABzslZ9dCb5PjiFxLbT9pg9/R4wmiEikS0RWpXLFZHiXbNvu+qruhmi6o0B9FtX7/Gpl5F/xy6YrC+uWrKVSdGbqEgawpex1/OZa6T3tmvcWTxjugMnJiJDtQ23EyNDWbBPa/9+NZ4Dpb4gvr39xZT5bej+u9u3o8dSVx/svSdhN0URRR2T9r/C9JC3+KL9f5kbqlXxdqm+alv9CSLWKP3DUukOFH3iRYhabwsjtP1fw5zVuPLX4Erswd6BDwRc/238tax2Z5FQGThG7/QU7b6q1Aja7f+RAVZt0keRmqh125bmQuU+9sRoH3beCH2Nmj3rsNNwVwGlZAsGRfXulQvQzr2fyqiu7Ik/tcHto2xa0Po58162uTMYaNxJqlLBwQht+IOVUNRB18IobbKO01pDFPWDl1apd7tV+pQt5DLTzwy05ZBMOdGOwDD7g2sQK909iKcGBTd/Nb3PWIMvpL4UOoNMQzFfuM4g2/pvVBQs+3NRVZUqfQ28tNgwao9xQ/qW0KpjvIqLi2nXTvslTUtLo7i4iYF4QpzITKFw23K46tPD3/ZwFAUe2a0tv+F/mT+9KvbqlQPpnhpFQkQo943tThf/gf+N8d/OIyJJq7hd+0XDitpFr8Gg6xo+bo/zYcQjcO6zZKVGs+HJc7l1RL1tphoxWh84HbC1h1/VjsRu3v1AASpThrFPTcV0+u0N7qtnL212Y7zzAO85z2F+/9fZ605mraMjDNS6noyRifzloj4Ykrp5B2gr3c9FSfKrTPrNxo2v3g4hEdx6z19Qh9wIvcZDaETAum8RnhDm6YLOGKpV+s59XqsSJndn4qB0Xr1yoPeYlGTfpIyk6FCSL3ycFx2XsaX3fd7Lh2TG89HNp1A58FYuivqYOe4zWOIeyCm2fxGRmM7/XT2IMdnaVmF97evJVdMD1uhSUvvQf8jp2jjDbmO9l2d06cUpXeuF4h4XMnFQBuNHnALAl3rXIkBKtK9SFxEZBbctx3TTQpyq9nwjEvRKTGgE3L8FbluOzahV49zhid5dKwrVRBwh+mPWX1vQu75fCqntOpKRnAhJeiVnwZ+Jrssn7uwHmBt7nW8dNcAQFsU1p2jjAhVFG7PYu30Mu+zabfapKd6tiJ7u/gXhk/5N/w6+Dw2/q1pwSIoyUzThU0Ivf5s6JZJoaulSrk0cSChdS4Jeednp9lWdvgy5MOApLNivd01+90jA5eVqFCX42pzoPshwwxZMOFna83E+j7iCC2zPscKtjVPLPOMKSqN7k+Cs95556p3UdT2PMbZ/UKZGcY/pSwBKXFHaRJ68ZVBXzl6jL+AOL/6IKL8uv/muYcxvdwf2uK586TqD0LDAbudp24awq9QXmLaonVjr1v42vjWOpHLQLax1d2Owoo39+rlaOx+FNSpvL93t7XJ0WauJqj8Gy+2Abd+xv8pKB9XXrdnXkEeE3TdW7nrj33k45DEq1CjilGrSlYPcaPqOq00/eqt3vnMbzUFisUV35Mdly3h/+R7vosinmveguKxHtHRHMLTZ4HpFUXxl90bMnDmT7OxssrOzKSlpuOCjEMe11N7Qo/FZg8es/SAYcBV01D+l6pWDCQPTWXDfCAyGZi4l4lldOuscOOW2pm83ZDJMaLjgJAYjjHrM+/hG/8e9fQXc1PjyDq9fNYh3pmQH7NnnX7UjJXDQdN9zb+Sjm4dz9bCGXbEjT/WNeypWEzizezL/6fsh/S5/Qluk99F9vsDomRE18jG46JXAO+oykrvafeSbmRmVQqeUeJSL/glXaF23Z3VP5rIhGYzqkUxE/W7ba2fD+X/XAqNeJVQUhQkDfTs6REbHe8NbQqSZQZ2S6HLpk1xznm98U8cE7Y38vrHd+d89ZzL9kn5Eh4VQRgwJkWbOzEomMVMLm4rlAJVxfXhqQh9tN4nUfpCYpQXoB7dDu/7e+331pvPooN83Zz0M133lC7dRyXx97jL+5ZqA2WRgxrWDURTFO/kkItSo/T6HxXjHHHnGHmpPLBGiU3HowQu33VvxunzkYEKi9e7yyHrBy7O+n/+esxGJ2hjGsl3aFmU9LqDa6qDcb7utiwd1YEhH7XeuSl/odFCHOEqJxq4ayVeTWOjWqnjD+vakQ0IEc+/wVW679j8DgNO7JXrHd1qUCDoZDhBhaziQ37/i5Tr7KRh+q3aeISAQVivRfMFoVBROcczgF7+JErGOErrpC//+3+YwXl+8g81qJvfZb+cZxzVk9D4Fa1wj264ldMZx2X8pIZ4V7t5k6DM8V1dG8putE1RpFcz/7Q3h0zjtw0OKoyBgrTtnVDp37D6d85wvcb/jdsLC9Z+TYuT9pPv5rSyC3X6biN9sv59P9QrjsuQrOKt7MtWxvurvJrcWem2E8u8lO3GbtOCl2ix0i2v4FPj4Cr5es5sOSgnVxnjcqsIAw05Crb7gtavWTFRYCOVqNAlKtfdcAdynPBxwd0b9g+F+c2d6KvvYunoJXX97jHdCXuDxwjuYbPy+zbsbWzV4paamUlSkjREoKioiJSWlydtOmzaNnJwccnJySPb7JCiEOAyTGSbOgMnzAoNFEz6Zdgrz7z6j6RsMvr7hTgTHKqUXZAxp9KpIs4nRPestLOpX4SJZD17XfAGXvo0hPJbTuiY1/kEu3tdttF+NJzLUyLOXD+fsvu21BXr9q3eeLtRhNze6nVbvHj0p0ZeNCJiZ6mmi0cA/LhvAuzcMa3649WeOJi1Ge55JUaEoisIlgzNIjDIzskey9zE8/0eEmrhyWEd6t9Oeg9Hz/FP7QKhWRRo16hyuPzUT+k2C25Zq1dbG+J+70Y97l0bxsIfGoWLgvL5pnNdXCxo3npEJQIxfpU+J1icPxDQ8Pz+lTWaDuzN5KaO1kDXhX0QNn6yHMEXrMvfnqXj57deKoviqn11Hg6JQbXVSofqC10Pn9aRPemBV9ophHVEx8JzzGj50ns0/Qu9kFDM5o4dfoO9xASgGumRoz8HkN2GmVvGbBRkbGPDzVO0+VMXANad01gL2UG2fVpvBN5bsjLqXeD/pAZQnylnwwGjfBAljOJHlm8ky5LNfjWdVkRYWbzmrC0UkYj7rbqLDQ1Hqdcl7ROkLFq9xa2HPoprJU9P4d65vkk18l0Gcc/NzbEg4hyEGrTI1v+NDrD3lFdZmaJXfXQe1alS0Wx8nNfxWrr/zCX18pk+hmsiK2AsYZJ1BSLu+xIaHcPNNvmrzXn28nY0QSi12Cuu0gG60V3JqZOMLKS/5YT4dlBKsCT3YpbZjvOFXb9csaF2pkWYT5UQRRw1d9R0OBlln4MwcGXBfnsrqTqUT3QyF3F/+DJ33zPaOqzvNsLnNB9i3avAaP348s2bNAmDWrFlMmDDhMEcIIY5a/WDRhFO6JNKnfSNrNp37HJxyO3QPUmXuaHm2lso6WwsUhxIWA6P+zE8RY1nq7keo6RAveaP+DA/uaDhWTXfLWV3oOVDff3NoIwsBHy3P5vLGEO9ehuZ67Xxn8lBynz2//pEA/GVcb/pnxPrGXRmM0EGfNdh+UKPHBIhs+gOwh2cTas8uEQAPntOD9X89hxi/LuF2GZ2bvM/rLxzFjB5vc/4wPUgNulab4RuZpAVdY71xR55w225g4OWe/fg6aT+LAR3iAtY+izKbvAHWIz0unDtHdSN25F289ec7WPrYuXz18EQi/cblccWH8OcDmPWFmE1+4blje7+AVm+9v81qJptNvVCu8S3g6lmu4oIhWexwt2e7O51KoshMigRFoVNiJB9F38AM5zhys5/EaC3nEuNS9hq0CuhtI7vy6Pk9mXfXGTx4jh64Op3KS45JXBc3K+DxPSF/nVvrxs9VMwCF9W59qZnOo/nbTZOIjwzFFuebMTu4VxaDzrsBR0RgSPbsPOGpNHZK0HcqiNSqx24MtI+PpJwYOifp5z0+U1t3EF8A/Dr8YgAW79QqydN5jQkH3/Q+Tr6axBj3v3CqBm40fktXpZCI1G687ryYDkoJql+Vu4YIosxGLGHtMCtOxhuXU6ZGUU4MyfWCYZcYrZq32qodn+wu4Tr7owywzmR3x0sZYtiOpa7eWLNWFrTgddVVV3Hqqaeybds2MjIyePvtt3n00UdZuHAhWVlZ/PDDDzz66KPBenghxLGKSoHznj/q9dVaXGq/wBmhzTXiISznv04JcfRMO0QQNZq0HQaaYDAohF/wDNyxCvpf1uTtjtjkr+EubSHIa/WxSd1SAtf3MhiUJidC9E2P5es7zwgMEd3P1wZXJzVeJfH6Uz7cs+6wTfQsHNojzdcuRVF8y6l4JGVBTEZghVKXEh3GG9cMCRy7B9D30gZLkwCQ0BluXgx9Lwm83DPRoYMWBJ65uC/z7h4RcBNFUbhrdDeevMjXTfnguT24b2x3EiJDMRkNDZd6MRjAGOLdUNto9AWv/h31btC4TpAcOPu6VI3m6ZRXoJvfcjj6uKbY2DjOtf+dC+zalkvxfo+Z3j6D6c6rqe4wGs+aV/mKFrwGZMShKAp902O9ldx+HZJ53XUJ1409RZtAU6/ytlbN4k3nBdzt0DamViMSeXfQ55iu9e2zqCT7ZqDGJWjP6dSugbOvQ06/Qxu3OeBqwLe12tJh/+Yv/Zfw5wt7Uad31XmDF2C4djb9rW9SSiyZ1o8IGaD9jcxarQ0VSqi33IYZBzvt8Sx0D+Ec42pilFrCU7ow130Go+wvo9yVA/dt5n99XsKNAbvTzRVTH8RuimaAYReOOC1o1v+7GN5Bq34uKNUC5SLXIH5x96eSKCrbn0G0UodauI621HBqQwv5+OOPG7180aJFwXpIIcSJ7NZf4Ag3E/a4oF878qZfePgbHk5YrPavJZmjvYPaz+2TRu6z5x96tmlzDLsZht7UYH25Rh+7Ga4e3onEKDPn921kj1J/Zz2sjXE6En0vaRiuPNIb6Y6+4EUY8bC37WEhRnq3bxioHzjnMKGzCU49ePlXvJSOp2hB9qpP4OD2gNu7jOFEh9V7Kw3V3vzjYuO9M1KBgKVpnrukH12SIxncqxsMvg7WvE9hRHeohQEdGv6OZSZFsvO5C7Sxkj13EbBAKeDCyLx2d3Je5wS+WlvAr4+ObvB7FN55OPpuV4RHa5XdC/u1o++DsYx8cYl2eUSkNm5T53lu4eYQnr5Eq6Cu2VvOWqBdnF/ANhi9lUeTQeGULol8smoflrrGtw77n0sLzmuHv0z+iue52fQ/DPqSOflqsvfvInXYpbD6V34vqCSjXRpc9CIsmc7uxFFQHNglzBn3Y8i+A379jR3uNMbZnmGL6lsIOrTbGP6z83HOSc6iLQUteAkhRIvSpqi1dSuC7phDF7T4uTIaFC7o14zN00MjtH/BFBLW+Jp2V3zYcID+URisD8z3bEsEaNW1+zdrX+sbSTPiERgyhZjXNgeMcwO85yA0Mhb0bYVeumwAF/b3ncOEyFAePk+vnl30GgybxsUhnUjaWUG72HqLF+u8E1SaqEJ7Jgk8dkGvRq/v3aMnqmLUxk/py84oihKwMGx9ngkf/lv+PDexH6N6pNAjtfHgvumpczGbjMy+9VSq66yUvBtLst+aW4OsM4iITmT1PSOICjPRc+nV1HQYxX29J/DYBfsw+P3u9s/QQmimp7o24EoYcCW5K/bAxo0kRIbAtJ+0KmNSFlGqismg4HSrFIT3wFXrWzy1d7dO9O4WOBi/LUjwEkIIcfzrNa5F7mZAhzi2Pn1eIwsK6+I7awu5thsAMe2ZfomRjvWDi2dmccZQuqf+Tlps+KG3KVMUSOtHBnBl4uHHZdb3ybRTsDsPv0SCoihw5ypY+SbEZwZe3oTbR3WjuNrGZUN8S1LERYQ2uaMHoO2pC2SlRgPRTG3/LmF7lvCv0NcA6NqpE3eM6kaiPmbwxwdHkxoTBiYj084KXHImxGhg3l1nkBwdOJbriuwOWGxOppyWCX4/K0VRCAsxUmNzMmFgOk+O78PnOfuwNuP8tBZFVY+ydt+KsrOzyclpetdzIYQQotXUHNBmY55AFdj//LQTt6oN7D9a8zYU4nKrAcukAOwqqeHh2Rv4/MA4rdr2ZGUT99AyMh+dD8Cn005pegeRIDtUbpGKlxBCCHEkog4/E/R4c0szFjc+nHH92zd6eZfkKGbfdhrUbPOtlxdEWSlR5B6oYVjnxmcotzUJXkIIIYQIvqhkIPjrcs65w7dI8R+RBC8hhBBCnDACllb5A2qzLYOEEEIIIU42EryEEEIIIVqJBC8hhBBCiFYiwUsIIYQQopVI8BJCCCGEaCUSvIQQQgghWokELyGEEEKIViLBSwghhBCilUjwEkIIIYRoJRK8hBBCCCFaiaKqqtrWjTicpKQkMjMzg/oYJSUlJCcHfw+pE5mcw2Mn5/DYyTk8dnIOj52cw2N3PJ/DvLw8Dh482Oh1x0Xwag3Z2dnk5OS0dTOOa3IOj52cw2Mn5/DYyTk8dnIOj92Jeg6lq1EIIYQQopVI8BJCCCGEaCUSvHTTpk1r6yYc9+QcHjs5h8dOzuGxk3N47OQcHrsT9RzKGC8hhBBCiFYiFS8hhBBCiFYiwQv47rvv6NGjB926dWP69Olt3Zw/rBtvvJGUlBT69u3rvaysrIyxY8eSlZXF2LFjKS8vB0BVVe6++266detG//79WbNmTVs1+w9j3759jBo1it69e9OnTx9effVVQM7hkbBarQwbNowBAwbQp08fnnjiCQB2797N8OHD6datG1dccQV2ux0Am83GFVdcQbdu3Rg+fDh5eXlt2Po/FpfLxaBBgxg3bhwg5/BIZWZm0q9fPwYOHEh2djYgf8tHqqKigkmTJtGzZ0969erF8uXLT45zqJ7knE6n2qVLF3Xnzp2qzWZT+/fvr27atKmtm/WH9NNPP6mrV69W+/Tp473soYceUp9//nlVVVX1+eefVx9++GFVVVV1/vz56nnnnae63W51+fLl6rBhw9qkzX8khYWF6urVq1VVVdWqqio1KytL3bRpk5zDI+B2u9Xq6mpVVVXVbrerw4YNU5cvX65edtll6scff6yqqqrecsst6htvvKGqqqr+61//Um+55RZVVVX1448/Vi+//PK2afgf0EsvvaReddVV6oUXXqiqqirn8Ah16tRJLSkpCbhM/paPzPXXX6+++eabqqqqqs1mU8vLy0+Kc3jSB69ff/1VPeecc7zfP/fcc+pzzz3Xhi36Y9u9e3dA8OrevbtaWFioqqoWLLp3766qqqpOmzZN/eijjxq9ndCMHz9eXbBggZzDo2SxWNRBgwapK1asUBMTE1WHw6GqauDf9DnnnKP++uuvqqqqqsPhUBMTE1W3291mbf6j2Ldvnzp69Gh10aJF6oUXXqi63W45h0eoseAlf8vNV1FRoWZmZjb4XToZzuFJ39VYUFBAhw4dvN9nZGRQUFDQhi06vhQXF9OuXTsA0tLSKC4uBuS8Hk5eXh5r165l+PDhcg6PkMvlYuDAgaSkpDB27Fi6du1KXFwcJpMJCDxP/ufQZDIRGxtLaWlpm7X9j+Lee+/lhRdewGDQ3gJKS0vlHB4hRVE455xzGDJkCDNnzgTk9fBI7N69m+TkZG644QYGDRrETTfdhMViOSnO4UkfvETLURQFRVHauhl/eDU1NVx66aW88sorxMTEBFwn5/DwjEYj69atIz8/n5UrV7J169a2btJxZd68eaSkpDBkyJC2bspxbenSpaxZs4Zvv/2Wf/3rX/z8888B18vf8qE5nU7WrFnDbbfdxtq1a4mMjGwwxvpEPYcnffBKT09n37593u/z8/NJT09vwxYdX1JTUykqKgKgqKiIlJQUQM5rUxwOB5deeinXXHMNl1xyCSDn8GjFxcUxatQoli9fTkVFBU6nEwg8T/7n0Ol0UllZSWJiYpu1+Y9g2bJlfP3112RmZnLllVeyePFi7rnnHjmHR8hzflJSUpg4cSIrV66Uv+UjkJGRQUZGBsOHDwdg0qRJrFmz5qQ4hyd98Bo6dCi5ubns3r0bu93OJ598wvjx49u6WceN8ePHM2vWLABmzZrFhAkTvJe///77qKrKihUriI2N9ZaPT1aqqjJ16lR69erF/fff771czmHzlZSUUFFRAUBdXR0LFy6kV69ejBo1itmzZwMNz6Hn3M6ePZvRo0efkJ+gj8Tzzz9Pfn4+eXl5fPLJJ4wePZoPP/xQzuERsFgsVFdXe79esGABffv2lb/lI5CWlkaHDh3Ytm0bAIsWLaJ3794nxzls0xFmfxDz589Xs7Ky1C5duqjPPPNMWzfnD+vKK69U09LSVJPJpKanp6tvvfWWevDgQXX06NFqt27d1DFjxqilpaWqqmqzz26//Xa1S5cuat++fdVVq1a1cevb3i+//KICar9+/dQBAwaoAwYMUOfPny/n8AisX79eHThwoNqvXz+1T58+6t/+9jdVVVV1586d6tChQ9WuXbuqkyZNUq1Wq6qqqlpXV6dOmjRJ7dq1qzp06FB1586dbdn8P5wff/zRO6tRzmHz7dy5U+3fv7/av39/tXfv3t73DflbPjJr165VhwwZovbr10+dMGGCWlZWdlKcQ1m5XgghhBCilZz0XY1CCCGEEK1FgpcQQgghRCuR4CWEEEII0UokeAkhhBBCtBIJXkIIIYQQrUSClxDiD0tRFK699lrv906nk+TkZMaNGwfA119/3WC16/oKCwuZNGkSAO+99x533nnnEbXhueeeO+xtpkyZ4l0DSwghDkWClxDiDysyMpKNGzdSV1cHwMKFCwNWqx4/fjyPPvroIe+jffv2xxSKmhO8hBCiuSR4CSH+0C644ALmz58PwMcff8xVV13lvc6/gjVlyhTuvvtuTjvtNLp06eINW3l5efTt29d7zL59+xg5ciRZWVn87W9/815+8cUXM2TIEPr06ePd9PjRRx+lrq6OgQMHcs011wDw/vvv079/fwYMGMB1113nPf7nn39u8NhCCFGfBC8hxB/alVdeySeffILVamXDhg3evd0aU1RUxNKlS5k3b16TlbCVK1fyxRdfsGHDBj7//HNycnIAeOedd1i9ejU5OTm89tprlJaWMn36dMLDw1m3bh0ffvghmzZt4plnnmHx4sWsX7+eV1999YgeWwghJHgJIf7Q+vfvT15eHh9//DEXXHDBIW978cUXYzAY6N27N8XFxY3eZuzYsSQmJhIeHs4ll1zC0qVLAXjttdcYMGAAp5xyCvv27SM3N7fBsYsXL+ayyy4jKSkJgISEhCN6bCGEMLV1A4QQ4nDGjx/Pgw8+yJIlSygtLW3ydmaz2ft1U7uh1d/gWVEUlixZwg8//MDy5cuJiIhg5MiRWK3WI2pjcx5bCCGk4iWE+MO78cYbeeKJJ+jXr98x39fChQspKyujrq6OOXPmcPrpp1NZWUl8fDwRERFs3bqVFStWeG8fEhKCw+EAYPTo0Xz++efe8FdWVnbM7RFCnFyk4iWE+MPLyMjg7rvvbpH7GjZsGJdeein5+flce+21ZGdn069fP2bMmEGvXr3o0aMHp5xyivf206ZNo3///gwePJgPP/yQxx9/nBEjRmA0Ghk0aBDvvfdei7RLCHFyUFSpiQshhBBCtArpahRCCCGEaCUSvIQQQgghWokELyGEEEKIViLBSwghhBCilUjwEkIIIYRoJRK8hBBCCCFaiQQvIYQQQohWIsFLCCGEEKKV/D/n3w5Bg+X8AgAAAABJRU5ErkJggg=="
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "_Pk_EScnpkpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taking a look at the training / test loss, the process is somewhat noisy. This could be a result of a variety of things: minibatch gradient descent is the obvious one, but the use of improper targets likely also contributes. By encouraging the correct class to fire at every time step, the loss function conflicts with the reset mechanism that tries to prevent this."
      ],
      "metadata": {
        "id": "g-Gd84OAl1rB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 Test Set Accuracy\n",
        "This function iterates over all minibatches to obtain a measure of accuracy over the full 10,000 samples in the test set."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Z3f0vBnBpkpk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "total = 0\r\n",
        "correct = 0\r\n",
        "\r\n",
        "# drop_last switched to False to keep all samples\r\n",
        "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=False)\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "  net.eval()\r\n",
        "  for data in test_loader:\r\n",
        "    images, labels = data\r\n",
        "    images = images.to(device)\r\n",
        "    labels = labels.to(device)\r\n",
        "\r\n",
        "    batch_size = images.size(0)  # the final batch has a different size so must be updated\r\n",
        "    outputs, _ = net(images.view(batch_size, -1))\r\n",
        "\r\n",
        "    _, predicted = outputs.sum(dim=0).max(1)\r\n",
        "    total += labels.size(0)\r\n",
        "    correct += (predicted == labels).sum().item()\r\n",
        "\r\n",
        "print(f\"Total correctly classified test set images: {correct}/{total}\")\r\n",
        "print(f\"Test Set Accuracy: {100 * correct / total}%\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total correctly classified test set images: 9367/10000\n",
            "Test Set Accuracy: 93.67%\n"
          ]
        }
      ],
      "metadata": {
        "id": "F5Rb4xHGndQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voila! That's it for static MNIST. Feel free to tweak the network parameters, hyperparameters, decay rate, using a learning rate scheduler etc. to see if you can improve the network performance. "
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "TBIXau4Zpkpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "Now you know how to construct and train a fully-connected network on a static dataset. The spiking neurons can actually be adapted to other layer types, including convolutions and skip connections. Armed with this knowledge, you should now be able to build many different types of SNNs.\n",
        "\n",
        "In the next tutorial, you will learn how to train a spiking convolutional network using a time-varying spiking dataset."
      ],
      "metadata": {
        "id": "s0dAgWUt2o6E"
      }
    }
  ]
}