{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47d5313e-c29d-4581-a9c7-a45122337069",
   "metadata": {},
   "source": [
    "# snnTorch - Tutorial 3\n",
    "### By Jason K. Eshraghian and Gregor Lenz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93694d9-0f0a-46a0-b17f-c04ac9b73a63",
   "metadata": {},
   "source": [
    "# Neuromorphic datasets and introduction to Tonic\n",
    "Now we're going to look at how we can use datasets that were recorded with a neuromorphic camera. For that we make use of [Tonic](https://github.com/neuromorphs/tonic), which works much like PyTorch vision. We can simply install the package from pypi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86102f99-5955-43d6-ba80-db6ed1561733",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tonic==1.0.4 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404c438f-82b2-4041-a7ea-88ecdd17f4cc",
   "metadata": {},
   "source": [
    "Let's start by loading the neuromorphic version of the MNIST dataset which is called [N-MNIST](https://tonic.readthedocs.io/en/latest/datasets.html#n-mnist). We can have a look at some raw events to get a feeling what we're working with. The raw output is an NxD array of events. Every row is one event and the columns are for x and y coordinates, timestamp and polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d286ef9-5fe6-4578-a686-91559a1f81d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[( 5, 23,    489, 0) (21, 13,   1790, 0) ( 1, 33,   5131, 1) ...\n",
      " (13, 14, 307497, 0) (18, 30, 308261, 1) ( 7, 15, 308966, 1)]\n"
     ]
    }
   ],
   "source": [
    "import tonic\n",
    "\n",
    "dataset = tonic.datasets.NMNIST(save_to='./data', train=True)\n",
    "events, target = dataset[0]\n",
    "print(events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dfd2fb-7ea7-483f-bef0-ff3d444df44a",
   "metadata": {},
   "source": [
    "If we were to accumulate those events over time and plot the bins as images, it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d6a0b0-2a73-4dbe-9576-d06c251f0fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tonic.utils.plot_event_grid(events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bcc031-d11a-4471-b3aa-335eec76d7ad",
   "metadata": {},
   "source": [
    "Our neural network doesn't just take as an input an array of raw events. We want to convert the raw data to a representation that is suitable, such as a tensor. We can choose a set of transforms to apply to our data before feeding it to our network. The neuromorphic camera sensor has a temporal resolution of microseconds, which when converted into a dense representation ends up in a very large tensor. That is why we bin events into a smaller number of frames using the [ToFrame transformation](https://tonic.readthedocs.io/en/latest/reference/transformations.html#frames), which reduces temporal precision but also allows us to work with it in a dense format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30f249be-8a65-4c1c-a21c-d561e904b4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tonic.transforms as transforms\n",
    "\n",
    "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
    "frame_transform = transforms.Compose([transforms.Denoise(filter_time=10000), \n",
    "                                      transforms.ToFrame(sensor_size=sensor_size, \n",
    "                                                         time_window=1000)\n",
    "                                     ])\n",
    "\n",
    "trainset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=True)\n",
    "testset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70be0b77-9405-44f6-af49-dfc4d49566c6",
   "metadata": {},
   "source": [
    "The original data is stored in a format that is slow to read. To speed up dataloading, we can make use of disk caching. That means that once files are loaded from the original file, they are written to disk in an efficient format in our cache directory. Let's compare some file reading speeds to read 100 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a6bf2a2-ff9f-4cdc-8cb3-02a1a9d71a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_simple():\n",
    "    for i in range(100):\n",
    "        events, target = trainset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a9d3b28-b303-4a17-be78-b9918911a7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.65 s ± 13.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TimeitResult : 1.65 s ± 13.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit -o load_sample_simple()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b957b32f-d76b-42c0-8c1c-b6b63e84e2ef",
   "metadata": {},
   "source": [
    "Let's try to decrease the time to read 100 samples by using a PyTorch DataLoader as well as disk caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f47e0798-5259-491a-9d3a-59b13b1b0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tonic import CachedDataset\n",
    "\n",
    "cached_trainset = CachedDataset(trainset, cache_path='./cache/nmnist/train')\n",
    "cached_dataloader = DataLoader(cached_trainset, num_workers=4)\n",
    "\n",
    "def load_sample_cached():\n",
    "    for i, (events, target) in enumerate(iter(cached_dataloader)):\n",
    "        if i > 99: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17a0219b-4d15-4f0b-b5be-8c728b5e24a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543 ms ± 9.09 ms per loop (mean ± std. dev. of 20 runs, 1 loop each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TimeitResult : 543 ms ± 9.09 ms per loop (mean ± std. dev. of 20 runs, 1 loop each)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit -o -r 20 load_sample_cached()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3831428d-0511-4fde-84d9-11d08fa45df7",
   "metadata": {},
   "source": [
    "Now that we've reduced our loading time to a third, we also want to use batching to make efficient use of the GPU. Because event recordings have different lengths, we are going to provide a specific collation function that takes care of how we make sure that all samples in a batch have the same dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b35c7cd-d292-47cd-9203-7f31aa7f7207",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "trainloader = DataLoader(cached_trainset, num_workers=4, batch_size=batch_size, collate_fn=tonic.collation.PadTensors())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14b9af4f-141e-4301-8451-445957ec8707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_batched():\n",
    "    events, target = next(iter(cached_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dc4b27a-63ac-4edc-94e9-589d548c4769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 ms ± 1.41 ms per loop (mean ± std. dev. of 10 runs, 10 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TimeitResult : 102 ms ± 1.41 ms per loop (mean ± std. dev. of 10 runs, 10 loops each)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit -o -r 10 load_sample_batched()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82a7afd-c011-4cd6-ba04-1e7cd438bc1f",
   "metadata": {},
   "source": [
    "Using disk caching and a PyTorch dataloader with multithreading and batching support we have reduced loading times to less than a tenth per sample in comparison to naively iterating over the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ded1bd9-e2f1-479e-899c-c6c2652e6fc9",
   "metadata": {},
   "source": [
    "# Training our network using frames created from events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be82d75-69ef-4c1b-ad85-4eca84c73ccf",
   "metadata": {},
   "source": [
    "Now let's actually train a network on the N-MNIST classification task. We start by defining our caching wrappers and dataloaders. While doing that, we're also going to apply some augmentations to the training data. The samples we receive from the cached dataset are frames, so we can make use of PyTorch Vision to apply whatever random transform we would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace6cd0b-7b56-4422-b3bd-23bac65db9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "transform = tonic.transforms.Compose([torch.from_numpy,\n",
    "                                      torchvision.transforms.RandomRotation([-10,10])])\n",
    "\n",
    "cached_trainset = CachedDataset(trainset, transform=transform, cache_path='./cache/nmnist/train')\n",
    "\n",
    "# no augmentations for the testset\n",
    "cached_testset = CachedDataset(testset, cache_path='./cache/nmnist/test')\n",
    "\n",
    "batch_size = 128\n",
    "trainloader = DataLoader(cached_trainset, num_workers=4, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(), shuffle=True)\n",
    "testloader = DataLoader(cached_testset, num_workers=4, batch_size=batch_size, collate_fn=tonic.collation.PadTensors())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528fe384-a365-4b53-bbfc-ed4dd261d222",
   "metadata": {},
   "source": [
    "A mini-batch now has the dimensions (time steps, batch size, channels, height, width). The number of time steps will be the one of the longest recording in the mini-batch, as all other samples have been padded with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e37337-ad4a-43d5-b429-81a18de5148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_tensor, target = next(iter(trainloader))\n",
    "print(event_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae5d4b-2bb3-4191-9f96-04b3c6ba4c41",
   "metadata": {},
   "source": [
    "## Defining our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107cb645-0227-4290-9e1b-25d6ae7eac87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23569dfc-e4a7-490f-8a68-c9ade5e03028",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a69416ee-bc8e-4f17-b6eb-a2ba396a08f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (event_tensors, targets) in enumerate(iter(trainloader)):\n",
    "    # train here\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
